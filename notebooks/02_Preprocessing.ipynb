{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afd5a0d",
   "metadata": {},
   "source": [
    "============================================================\n",
    "### 02 - NETTOYAGE DES DONN√âES\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715c532",
   "metadata": {},
   "source": [
    "**Objectif** : Traiter les anomalies identifi√©es lors de l'AED\n",
    "- Suppression des valeurs impossibles\n",
    "- Imputation des valeurs manquantes\n",
    "- Gestion des outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7348bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= IMPORTS =============\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca92e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c1610979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/donnees_recrutement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "18275a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne Unnamed: 0\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660d499",
   "metadata": {},
   "source": [
    "#### SECTION 1 : NETTOYAGE DES DONN√âES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba31f2b",
   "metadata": {},
   "source": [
    "##### 1.1 TRAITEMENT DES VALEURS IMPOSSIBLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b4ecc1",
   "metadata": {},
   "source": [
    "**detections des valeurs impossibles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f4d0d939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Variable AGE :\n",
      "----------------------------------------\n",
      "   ‚Ä¢ √Çges N√âGATIFS (< 0) : 3\n",
      "   ‚Ä¢ √Çges TROP JEUNES (0-14) : 354\n",
      "   ‚Ä¢ TOTAL anomalies AGE : 357\n"
     ]
    }
   ],
   "source": [
    "# --- AGE : d√©tection ---\n",
    "print(\"\\nüìå Variable AGE :\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Crit√®res pour √¢ge impossible/anormal\n",
    "age_negatif = df['age'] < 0\n",
    "age_trop_jeune = (df['age'] >= 0) & (df['age'] < 15)\n",
    "\n",
    "\n",
    "print(f\"   ‚Ä¢ √Çges N√âGATIFS (< 0) : {age_negatif.sum()}\")\n",
    "print(f\"   ‚Ä¢ √Çges TROP JEUNES (0-14) : {age_trop_jeune.sum()}\")\n",
    "\n",
    "print(f\"   ‚Ä¢ TOTAL anomalies AGE : {(age_negatif | age_trop_jeune ).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "01ad0bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   cas avec age n√©gative :\n",
      "       age   exp   diplome  salaire specialite\n",
      "1842  -1.0   6.0       bac  38176.0  detective\n",
      "3968  -3.0  11.0  doctorat  26167.0     forage\n",
      "19605 -2.0  12.0  doctorat  27837.0   geologie\n"
     ]
    }
   ],
   "source": [
    "if age_negatif.sum() > 0:\n",
    "    print(f\"\\n   cas avec age n√©gative :\")\n",
    "    exemples_exp = df[age_negatif][['age', 'exp', 'diplome', 'salaire', 'specialite']].head(10)\n",
    "    print(exemples_exp.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81e83be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   cas avec age n√©gative :\n",
      "      age   exp   diplome  salaire   specialite\n",
      "112  14.0  13.0    master  34851.0       forage\n",
      "164  10.0   9.0    master  38767.0  archeologie\n",
      "211  12.0  10.0    master  31557.0    detective\n",
      "219  14.0   9.0   licence  32178.0    detective\n",
      "231  13.0  17.0  doctorat  29115.0     geologie\n",
      "258   8.0  10.0   licence  44530.0  archeologie\n",
      "270  13.0   7.0       bac  30800.0    detective\n",
      "290  14.0   7.0    master  41795.0       forage\n",
      "296   8.0  16.0    master  25825.0  archeologie\n",
      "396   8.0  11.0    master  36915.0     geologie\n"
     ]
    }
   ],
   "source": [
    "if age_trop_jeune.sum() > 0:\n",
    "    print(f\"\\n   cas avec age n√©gative :\")\n",
    "    exemples_exp = df[age_trop_jeune][['age', 'exp', 'diplome', 'salaire', 'specialite']].head(10)\n",
    "    print(exemples_exp.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc2b8c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üìå Variable EXP :\n",
      "----------------------------------------\n",
      "   ‚Ä¢ Exp√©riences N√âGATIVES (< 0) : 2\n"
     ]
    }
   ],
   "source": [
    "# --- EXP : d√©tection ---\n",
    "print(\"\\n\\nüìå Variable EXP :\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "exp_negatif = df['exp'] < 0\n",
    "\n",
    "print(f\"   ‚Ä¢ Exp√©riences N√âGATIVES (< 0) : {exp_negatif.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "830c6682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Cas avec EXP n√©gative :\n",
      "        age  exp  diplome  salaire specialite\n",
      "6025   36.0 -1.0   master  29241.0   geologie\n",
      "11284  34.0 -2.0  licence  51294.0  detective\n"
     ]
    }
   ],
   "source": [
    "if exp_negatif.sum() > 0:\n",
    "    print(f\"\\n   Cas avec EXP n√©gative :\")\n",
    "    exemples_exp = df[exp_negatif][['age', 'exp', 'diplome', 'salaire', 'specialite']].head(10)\n",
    "    print(exemples_exp.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44211f",
   "metadata": {},
   "source": [
    "**Imputations des ces valeurs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "307c3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer_par_profils_similaires(df, index, variable_a_imputer, tolerance_salaire=5000):\n",
    "    \"\"\"\n",
    "    Impute AGE ou EXP en se basant sur des profils similaires (dipl√¥me + salaire)\n",
    "    \n",
    "    Param√®tres :\n",
    "    - df : DataFrame\n",
    "    - index : index de la ligne √† imputer\n",
    "    - variable_a_imputer : 'age' ou 'exp'\n",
    "    - tolerance_salaire : √©cart acceptable pour le salaire (d√©faut: 5000)\n",
    "    \n",
    "    Retourne :\n",
    "    - valeur imput√©e (ou None si impossible)\n",
    "    \"\"\"\n",
    "    \n",
    "    # R√©cup√©rer dipl√¥me et salaire du candidat\n",
    "    diplome_cible = df.loc[index, 'diplome']\n",
    "    salaire_cible = df.loc[index, 'salaire']\n",
    "    specialite_cible=df.loc[index,'specialite']\n",
    "    \n",
    "    # Si salaire manquant, impossible d'utiliser cette m√©thode\n",
    "    if pd.isna(salaire_cible):\n",
    "        return None\n",
    "    \n",
    "    # Trouver les profils similaires\n",
    "    mask = (\n",
    "        (df['diplome'] == diplome_cible) &  # M√™me dipl√¥me\n",
    "        (df['specialite'] == specialite_cible) &\n",
    "        (df['salaire'] >= salaire_cible - tolerance_salaire) &  # Salaire proche\n",
    "        (df['salaire'] <= salaire_cible + tolerance_salaire) &\n",
    "        (df[variable_a_imputer] > 0) &  # Valeur valide\n",
    "        (df.index != index)  # Exclure la ligne elle-m√™me\n",
    "    )\n",
    "    \n",
    "    profils_similaires = df[mask]\n",
    "    \n",
    "    # Si aucun profil trouv√©, retourner None\n",
    "    if len(profils_similaires) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Retourner la m√©diane des profils similaires\n",
    "    return profils_similaires[variable_a_imputer].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fc33ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß CORRECTION des AGE n√©gatifs :\n",
      "\n",
      "   Index 1842: -1.0 ‚Üí 36.0\n",
      "   Index 3968: -3.0 ‚Üí 35.0\n",
      "   Index 19605: -2.0 ‚Üí 35.0\n",
      "\n",
      "   Corrig√©s par profils similaires : 3\n",
      "   Corrig√©s par fallback : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîß CORRECTION des AGE n√©gatifs :\\n\")\n",
    "\n",
    "nb_corriges = 0\n",
    "nb_echecs = 0\n",
    "\n",
    "for idx in df[age_negatif].index:\n",
    "    age_original = df.loc[idx, 'age']\n",
    "    \n",
    "    # Tenter l'imputation\n",
    "    valeur_imputee = imputer_par_profils_similaires(df, idx, 'age')\n",
    "    \n",
    "    if valeur_imputee is not None:\n",
    "        df.loc[idx, 'age'] = valeur_imputee\n",
    "        nb_corriges += 1\n",
    "        print(f\"   Index {idx}: {age_original:.1f} ‚Üí {valeur_imputee:.1f}\")\n",
    "    else:\n",
    "        # Fallback: m√©diane du dipl√¥me\n",
    "        diplome = df.loc[idx, 'diplome']\n",
    "        mean_diplome = df[(df['diplome'] == diplome) & (df['age'] > 0)]['age'].mean()\n",
    "        df.loc[idx, 'age'] = mean_diplome\n",
    "        nb_echecs += 1\n",
    "        print(f\"   Index {idx}: {age_original:.1f} ‚Üí {mean_diplome:.1f} (fallback)\")\n",
    "\n",
    "print(f\"\\n   Corrig√©s par profils similaires : {nb_corriges}\")\n",
    "print(f\"   Corrig√©s par fallback : {nb_echecs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "94c8f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß CORRECTION des EXP n√©gatifs :\n",
      "\n",
      "   Index 6025: -1.0 ‚Üí 10.0\n",
      "   Index 11284: -2.0 ‚Üí 9.0\n",
      "\n",
      "   Corrig√©s par profils similaires : 2\n",
      "   Corrig√©s par fallback : 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLICATION - EXP N√âGATIFS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüîß CORRECTION des EXP n√©gatifs :\\n\")\n",
    "\n",
    "nb_corriges = 0\n",
    "nb_echecs = 0\n",
    "\n",
    "for idx in df[exp_negatif].index:\n",
    "    exp_original = df.loc[idx, 'exp']\n",
    "    \n",
    "    # Tenter l'imputation\n",
    "    valeur_imputee = imputer_par_profils_similaires(df, idx, 'exp')\n",
    "    \n",
    "    if valeur_imputee is not None:\n",
    "        df.loc[idx, 'exp'] = valeur_imputee\n",
    "        nb_corriges += 1\n",
    "        print(f\"   Index {idx}: {exp_original:.1f} ‚Üí {valeur_imputee:.1f}\")\n",
    "    else:\n",
    "        # Fallback: m√©diane du dipl√¥me\n",
    "        diplome = df.loc[idx, 'diplome']\n",
    "        mean_diplome = df[(df['diplome'] == diplome) & (df['exp'] >= 0)]['exp'].mean()\n",
    "        df.loc[idx, 'exp'] = mean_diplome\n",
    "        nb_echecs += 1\n",
    "        print(f\"   Index {idx}: {exp_original:.1f} ‚Üí {mean_diplome:.1f} (fallback)\")\n",
    "\n",
    "print(f\"\\n   Corrig√©s par profils similaires : {nb_corriges}\")\n",
    "print(f\"   Corrig√©s par fallback : {nb_echecs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc35c5e",
   "metadata": {},
   "source": [
    "##### 1.2 TRAITEMENT DES VALEURS MANQUANTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df7b10",
   "metadata": {},
   "source": [
    "**Detections des valeurs manquantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "33774212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "D√âTECTION DES VALEURS MANQUANTES\n",
      "================================================================================\n",
      "\n",
      "R√©sum√© des valeurs manquantes :\n",
      "\n",
      "  Variable  Nb_manquants  Pourcentage\n",
      "      note           114        0.570\n",
      "   diplome           110        0.550\n",
      "     dispo           106        0.530\n",
      "   cheveux           103        0.515\n",
      "      sexe           100        0.500\n",
      "       exp            96        0.480\n",
      "   salaire            95        0.475\n",
      "specialite            93        0.465\n",
      "      date            91        0.455\n",
      "       age            91        0.455\n",
      "\n",
      "   Total de valeurs manquantes : 999 (0.416% du dataset)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"D√âTECTION DES VALEURS MANQUANTES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculer le nombre de valeurs manquantes par variable\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "missing_percent = (total_missing / total_cells) * 100\n",
    "\n",
    "\n",
    "# Cr√©er un DataFrame r√©capitulatif\n",
    "missing_df = pd.DataFrame({\n",
    "    'Variable': missing.index,\n",
    "    'Nb_manquants': missing.values,\n",
    "    'Pourcentage': missing_pct.values\n",
    "})\n",
    "\n",
    "# Filtrer uniquement les variables avec valeurs manquantes\n",
    "missing_df = missing_df[missing_df['Nb_manquants'] > 0].sort_values('Nb_manquants', ascending=False)\n",
    "\n",
    "print(\"\\nR√©sum√© des valeurs manquantes :\\n\")\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string(index=False))\n",
    "    print(f\"\\n   Total de valeurs manquantes : {missing_df['Nb_manquants'].sum()} ({missing_percent:.3f}% du dataset)\")\n",
    "\n",
    "else:\n",
    "    print(\"    Aucune valeur manquante d√©tect√©e !\")\n",
    "    print(\"\\n   ‚Üí Rien √† faire, passage √† l'√©tape suivante.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76ec38b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IDENTIFICATION DES TYPES DE VARIABLES\n",
      "================================================================================\n",
      "\n",
      "Variables num√©riques (5) : ['index', 'age', 'exp', 'salaire', 'note']\n",
      "Variables cat√©gorielles (5) : ['cheveux', 'sexe', 'diplome', 'specialite', 'dispo']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFICATION DES TYPES DE VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Variables num√©riques et cat√©gorielles\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Retirer la variable cible et la date des listes\n",
    "if 'embauche' in numeric_cols:\n",
    "    numeric_cols.remove('embauche')\n",
    "if 'date' in categorical_cols:\n",
    "    categorical_cols.remove('date')\n",
    "\n",
    "print(f\"\\nVariables num√©riques ({len(numeric_cols)}) : {numeric_cols}\")\n",
    "print(f\"Variables cat√©gorielles ({len(categorical_cols)}) : {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b0ddd4",
   "metadata": {},
   "source": [
    "**STRAT√âGIE 1 : SUPPRESSION DES VALEURS MANQUANTES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f1a94",
   "metadata": {},
   "source": [
    "Justification de la suppression : total valeurs maquantes < 1% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e5e65050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " R√©sultats :\n",
      "   ‚Ä¢ Lignes avant suppression : 20000\n",
      "   ‚Ä¢ Lignes apr√®s suppression : 19021\n",
      "   ‚Ä¢ Lignes supprim√©es : 979 (4.90%)\n",
      "   ‚Ä¢ Valeurs manquantes restantes : 0\n",
      "\n",
      "Dataset 1 cr√©√© : df_dropna (suppression)\n"
     ]
    }
   ],
   "source": [
    "df_dropna = df.dropna().copy()\n",
    "\n",
    "print(f\"\\n R√©sultats :\")\n",
    "print(f\"   ‚Ä¢ Lignes avant suppression : {len(df)}\")\n",
    "print(f\"   ‚Ä¢ Lignes apr√®s suppression : {len(df_dropna)}\")\n",
    "print(f\"   ‚Ä¢ Lignes supprim√©es : {len(df) - len(df_dropna)} ({(len(df) - len(df_dropna))/len(df)*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Valeurs manquantes restantes : {df_dropna.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nDataset 1 cr√©√© : df_dropna (suppression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d07f2",
   "metadata": {},
   "source": [
    "**STRAT√âGIE 2 : IMPUTATION M√âDIANE/MODE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7f177314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputation des variables NUM√âRIQUES par la M√âDIANE :\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ age             :  91 valeurs imput√©es avec m√©diane = 35.00\n",
      "   ‚Ä¢ exp             :  96 valeurs imput√©es avec m√©diane = 9.00\n",
      "   ‚Ä¢ salaire         :  95 valeurs imput√©es avec m√©diane = 34979.00\n",
      "   ‚Ä¢ note            : 114 valeurs imput√©es avec m√©diane = 75.08\n",
      "\n",
      "Imputation des variables CAT√âGORIELLES par le MODE :\n",
      "--------------------------------------------------------------------------------\n",
      "    cheveux         : 103 valeurs imput√©es avec mode = 'chatain'\n",
      "    sexe            : 100 valeurs imput√©es avec mode = 'M'\n",
      "    diplome         : 110 valeurs imput√©es avec mode = 'master'\n",
      "    specialite      :  93 valeurs imput√©es avec mode = 'geologie'\n",
      "    dispo           : 106 valeurs imput√©es avec mode = 'non'\n",
      "   date            :  91 valeurs imput√©es avec mode\n",
      "\n",
      "Valeurs manquantes restantes : 0\n",
      " Dataset 2 cr√©√© : df_median_mode (m√©diane/mode)\n"
     ]
    }
   ],
   "source": [
    "df_median_mode = df.copy()\n",
    "\n",
    "print(\"\\nImputation des variables NUM√âRIQUES par la M√âDIANE :\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if df_median_mode[col].isnull().sum() > 0:\n",
    "        median_value = df_median_mode[col].median()\n",
    "        missing_count = df_median_mode[col].isnull().sum()\n",
    "        df_median_mode[col].fillna(median_value, inplace=True)\n",
    "        print(f\"   ‚Ä¢ {col:15s} : {missing_count:3d} valeurs imput√©es avec m√©diane = {median_value:.2f}\")\n",
    "\n",
    "print(\"\\nImputation des variables CAT√âGORIELLES par le MODE :\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df_median_mode[col].isnull().sum() > 0:\n",
    "        mode_value = df_median_mode[col].mode()[0]\n",
    "        missing_count = df_median_mode[col].isnull().sum()\n",
    "        df_median_mode[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"    {col:15s} : {missing_count:3d} valeurs imput√©es avec mode = '{mode_value}'\")\n",
    "\n",
    "# Traitement de la colonne date (si manquante)\n",
    "if 'date' in df_median_mode.columns and df_median_mode['date'].isnull().sum() > 0:\n",
    "    mode_date = df_median_mode['date'].mode()[0]\n",
    "    missing_count = df_median_mode['date'].isnull().sum()\n",
    "    df_median_mode['date'].fillna(mode_date, inplace=True)\n",
    "    print(f\"   {'date':15s} : {missing_count:3d} valeurs imput√©es avec mode\")\n",
    "\n",
    "print(f\"\\nValeurs manquantes restantes : {df_median_mode.isnull().sum().sum()}\")\n",
    "print(\" Dataset 2 cr√©√© : df_median_mode (m√©diane/mode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7b87d",
   "metadata": {},
   "source": [
    "**STRAT√âGIE 3 : IMPUTATION PAR KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "73798e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "√âtape 1 : Pr√©paration des donn√©es pour KNN\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ cheveux         encod√© (4 cat√©gories)\n",
      "   ‚Ä¢ sexe            encod√© (2 cat√©gories)\n",
      "   ‚Ä¢ diplome         encod√© (4 cat√©gories)\n",
      "   ‚Ä¢ specialite      encod√© (4 cat√©gories)\n",
      "   ‚Ä¢ dispo           encod√© (2 cat√©gories)\n",
      "   ‚Ä¢ date            convertie en timestamp\n"
     ]
    }
   ],
   "source": [
    "df_knn = df.copy()\n",
    "\n",
    "# Pour KNN, on va s√©parer les colonnes cat√©gorielles et num√©riques\n",
    "print(\"\\n√âtape 1 : Pr√©paration des donn√©es pour KNN\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cr√©er une copie pour le travail\n",
    "df_for_knn = df_knn.copy()\n",
    "\n",
    "# Dictionnaire pour stocker les mappings\n",
    "col_mappings = {}\n",
    "\n",
    "# Encoder les colonnes cat√©gorielles\n",
    "for col in categorical_cols:\n",
    "    if col in df_for_knn.columns:\n",
    "        # Cr√©er un mapping personnalis√©\n",
    "        unique_vals = df_for_knn[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "        mapping[np.nan] = np.nan  # Garder les NaN\n",
    "        \n",
    "        col_mappings[col] = {v: k for k, v in mapping.items()}  # Inverse mapping pour le d√©codage\n",
    "        \n",
    "        df_for_knn[col] = df_for_knn[col].map(mapping)\n",
    "        print(f\"   ‚Ä¢ {col:15s} encod√© ({len(unique_vals)} cat√©gories)\")\n",
    "\n",
    "# Traiter la date\n",
    "if 'date' in df_for_knn.columns:\n",
    "    df_for_knn['date'] = pd.to_datetime(df_for_knn['date'], errors='coerce')\n",
    "    df_for_knn['date_numeric'] = df_for_knn['date'].astype('int64') / 10**9\n",
    "    df_for_knn = df_for_knn.drop('date', axis=1)\n",
    "    print(f\"   ‚Ä¢ {'date':15s} convertie en timestamp\")\n",
    "\n",
    "# S√©parer la variable cible\n",
    "embauche_col = df_for_knn['embauche'].copy()\n",
    "df_for_knn = df_for_knn.drop('embauche', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5a9e80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " √âtape 2 : Application de KNNImputer\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Param√®tres : n_neighbors=5, weights='uniform'\n",
      "   ‚Ä¢ Variables √† imputer : 11\n",
      " Imputation termin√©e\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n √âtape 2 : Application de KNNImputer\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Initialiser KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "\n",
    "print(f\"   ‚Ä¢ Param√®tres : n_neighbors=5, weights='uniform'\")\n",
    "print(f\"   ‚Ä¢ Variables √† imputer : {df_for_knn.shape[1]}\")\n",
    "\n",
    "# Appliquer KNN\n",
    "df_imputed_knn = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(df_for_knn),\n",
    "    columns=df_for_knn.columns,\n",
    "    index=df_for_knn.index\n",
    ")\n",
    "\n",
    "print(f\" Imputation termin√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b6a60fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " √âtape 3 : D√©codage des variables cat√©gorielles\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ cheveux         d√©cod√©\n",
      "   ‚Ä¢ sexe            d√©cod√©\n",
      "   ‚Ä¢ diplome         d√©cod√©\n",
      "   ‚Ä¢ specialite      d√©cod√©\n",
      "   ‚Ä¢ dispo           d√©cod√©\n",
      "   ‚Ä¢ date            reconvertie\n",
      "\n",
      " Valeurs manquantes restantes : 91\n",
      "Dataset 3 cr√©√© : df_knn (KNN imputation)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n √âtape 3 : D√©codage des variables cat√©gorielles\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# D√©coder les variables cat√©gorielles\n",
    "for col in categorical_cols:\n",
    "    if col in df_imputed_knn.columns:\n",
    "        # Arrondir pour obtenir des entiers valides\n",
    "        df_imputed_knn[col] = df_imputed_knn[col].round()\n",
    "        \n",
    "        # R√©cup√©rer le mapping inverse\n",
    "        inverse_mapping = col_mappings[col]\n",
    "        \n",
    "        # S'assurer que les valeurs sont dans la plage valide\n",
    "        valid_codes = list(inverse_mapping.keys())\n",
    "        valid_codes = [x for x in valid_codes if not pd.isna(x)]\n",
    "        min_code, max_code = min(valid_codes), max(valid_codes)\n",
    "        df_imputed_knn[col] = df_imputed_knn[col].clip(min_code, max_code)\n",
    "        \n",
    "        # D√©coder\n",
    "        df_imputed_knn[col] = df_imputed_knn[col].map(inverse_mapping)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ {col:15s} d√©cod√©\")\n",
    "\n",
    "# Reconvertir la date\n",
    "if 'date_numeric' in df_imputed_knn.columns:\n",
    "    df_imputed_knn['date'] = pd.to_datetime(df_imputed_knn['date_numeric'] * 10**9, unit='ns')\n",
    "    df_imputed_knn['date'] = df_imputed_knn['date'].dt.strftime('%Y-%m-%d')\n",
    "    df_imputed_knn = df_imputed_knn.drop('date_numeric', axis=1)\n",
    "    print(f\"   ‚Ä¢ {'date':15s} reconvertie\")\n",
    "\n",
    "# Ajouter la colonne embauche\n",
    "df_imputed_knn['embauche'] = embauche_col\n",
    "\n",
    "# R√©organiser les colonnes dans l'ordre original\n",
    "df_knn = df_imputed_knn[df.columns]\n",
    "\n",
    "print(f\"\\n Valeurs manquantes restantes : {df_knn.isnull().sum().sum()}\")\n",
    "print(\"Dataset 3 cr√©√© : df_knn (KNN imputation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e20864d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution de la variable cible 'embauche' (v√©rification) :\n",
      "================================================================================\n",
      "   Strat√©gie  Non embauch√©s (0)  Embauch√©s (1)  Ratio (0/1)\n",
      " Suppression              16842           2179         7.73\n",
      "M√©diane/Mode              17708           2292         7.73\n",
      "         KNN              17708           2292         7.73\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDistribution de la variable cible 'embauche' (v√©rification) :\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "embauche_comparison = pd.DataFrame({\n",
    "    'Strat√©gie': ['Suppression', 'M√©diane/Mode', 'KNN'],\n",
    "    'Non embauch√©s (0)': [\n",
    "        (df_dropna['embauche'] == 0).sum(),\n",
    "        (df_median_mode['embauche'] == 0).sum(),\n",
    "        (df_knn['embauche'] == 0).sum()\n",
    "    ],\n",
    "    'Embauch√©s (1)': [\n",
    "        (df_dropna['embauche'] == 1).sum(),\n",
    "        (df_median_mode['embauche'] == 1).sum(),\n",
    "        (df_knn['embauche'] == 1).sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "embauche_comparison['Ratio (0/1)'] = (embauche_comparison['Non embauch√©s (0)'] / \n",
    "                                       embauche_comparison['Embauch√©s (1)']).round(2)\n",
    "\n",
    "print(embauche_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d74b34b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAUVEGARDE DES 3 DATASETS PR√âPAR√âS\n",
      "================================================================================\n",
      "\n",
      "Datasets sauvegard√©s :\n",
      "   ‚Ä¢ df_dropna.csv      : 19021 lignes √ó 12 colonnes\n",
      "   ‚Ä¢ df_median_mode.csv : 20000 lignes √ó 12 colonnes\n",
      "   ‚Ä¢ df_knn.csv         : 20000 lignes √ó 12 colonnes\n"
     ]
    }
   ],
   "source": [
    "# ============= SAUVEGARDE DES 3 DATASETS =============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAUVEGARDE DES 3 DATASETS PR√âPAR√âS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sauvegarder les datasets\n",
    "df_dropna.to_csv('../data/df_dropna.csv', index=False)\n",
    "df_median_mode.to_csv('../data/df_median_mode.csv', index=False)\n",
    "df_knn.to_csv('../data/df_knn.csv', index=False)\n",
    "\n",
    "print(\"\\nDatasets sauvegard√©s :\")\n",
    "print(f\"   ‚Ä¢ df_dropna.csv      : {df_dropna.shape[0]} lignes √ó {df_dropna.shape[1]} colonnes\")\n",
    "print(f\"   ‚Ä¢ df_median_mode.csv : {df_median_mode.shape[0]} lignes √ó {df_median_mode.shape[1]} colonnes\")\n",
    "print(f\"   ‚Ä¢ df_knn.csv         : {df_knn.shape[0]} lignes √ó {df_knn.shape[1]} colonnes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac777b5",
   "metadata": {},
   "source": [
    "##### 1.3 GESTION DES OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f17f6",
   "metadata": {},
   "source": [
    "**Detectiions des outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "714e9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= FONCTION DE D√âTECTION DES OUTLIERS =============\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"\n",
    "    D√©tecte les outliers avec la m√©thode IQR (Interquartile Range)\n",
    "    Outliers = valeurs < Q1 - 1.5*IQR OU > Q3 + 1.5*IQR\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    \n",
    "    return {\n",
    "        'Q1': Q1,\n",
    "        'Q3': Q3,\n",
    "        'IQR': IQR,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'n_outliers': len(outliers),\n",
    "        'pct_outliers': (len(outliers) / len(df)) * 100,\n",
    "        'outlier_indices': outliers.index.tolist()\n",
    "    }\n",
    "\n",
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    \"\"\"\n",
    "    D√©tecte les outliers avec la m√©thode Z-score\n",
    "    Outliers = |Z-score| > threshold (g√©n√©ralement 3)\n",
    "    \"\"\"\n",
    "    z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "    outliers_mask = z_scores > threshold\n",
    "    n_outliers = outliers_mask.sum()\n",
    "    \n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'n_outliers': n_outliers,\n",
    "        'pct_outliers': (n_outliers / len(df)) * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fce99fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IDENTIFICATION DES OUTLIERS - Dataset de r√©f√©rence (df_dropna)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      " Variable : INDEX\n",
      "================================================================================\n",
      "\n",
      "M√©thode IQR (Interquartile Range) :\n",
      "   ‚Ä¢ Q1 (25e percentile)     : 4986.00\n",
      "   ‚Ä¢ Q3 (75e percentile)     : 14993.00\n",
      "   ‚Ä¢ IQR (Q3 - Q1)           : 10007.00\n",
      "   ‚Ä¢ Limite inf√©rieure       : -10024.50\n",
      "   ‚Ä¢ Limite sup√©rieure       : 30003.50\n",
      "   ‚Ä¢ Nombre d'outliers       : 0 (0.00%)\n",
      "\n",
      "M√©thode Z-Score (threshold = 3) :\n",
      "   ‚Ä¢ Nombre d'outliers       : 0 (0.00%)\n",
      "\n",
      "Statistiques descriptives :\n",
      "   ‚Ä¢ Moyenne                 : 9993.01\n",
      "   ‚Ä¢ M√©diane                 : 9989.00\n",
      "   ‚Ä¢ √âcart-type              : 5771.34\n",
      "   ‚Ä¢ Min                     : 0.00\n",
      "   ‚Ä¢ Max                     : 19999.00\n",
      "\n",
      "Niveau de pr√©sence d'outliers : FAIBLE\n",
      "\n",
      "================================================================================\n",
      " Variable : AGE\n",
      "================================================================================\n",
      "\n",
      "M√©thode IQR (Interquartile Range) :\n",
      "   ‚Ä¢ Q1 (25e percentile)     : 29.00\n",
      "   ‚Ä¢ Q3 (75e percentile)     : 41.00\n",
      "   ‚Ä¢ IQR (Q3 - Q1)           : 12.00\n",
      "   ‚Ä¢ Limite inf√©rieure       : 11.00\n",
      "   ‚Ä¢ Limite sup√©rieure       : 59.00\n",
      "   ‚Ä¢ Nombre d'outliers       : 203 (1.07%)\n",
      "\n",
      "M√©thode Z-Score (threshold = 3) :\n",
      "   ‚Ä¢ Nombre d'outliers       : 61 (0.32%)\n",
      "\n",
      "Statistiques descriptives :\n",
      "   ‚Ä¢ Moyenne                 : 35.00\n",
      "   ‚Ä¢ M√©diane                 : 35.00\n",
      "   ‚Ä¢ √âcart-type              : 9.60\n",
      "   ‚Ä¢ Min                     : 0.00\n",
      "   ‚Ä¢ Max                     : 72.00\n",
      "\n",
      "Niveau de pr√©sence d'outliers : FAIBLE\n",
      "\n",
      "================================================================================\n",
      " Variable : EXP\n",
      "================================================================================\n",
      "\n",
      "M√©thode IQR (Interquartile Range) :\n",
      "   ‚Ä¢ Q1 (25e percentile)     : 7.00\n",
      "   ‚Ä¢ Q3 (75e percentile)     : 12.00\n",
      "   ‚Ä¢ IQR (Q3 - Q1)           : 5.00\n",
      "   ‚Ä¢ Limite inf√©rieure       : -0.50\n",
      "   ‚Ä¢ Limite sup√©rieure       : 19.50\n",
      "   ‚Ä¢ Nombre d'outliers       : 6 (0.03%)\n",
      "\n",
      "M√©thode Z-Score (threshold = 3) :\n",
      "   ‚Ä¢ Nombre d'outliers       : 45 (0.24%)\n",
      "\n",
      "Statistiques descriptives :\n",
      "   ‚Ä¢ Moyenne                 : 9.50\n",
      "   ‚Ä¢ M√©diane                 : 9.00\n",
      "   ‚Ä¢ √âcart-type              : 3.01\n",
      "   ‚Ä¢ Min                     : 0.00\n",
      "   ‚Ä¢ Max                     : 23.00\n",
      "\n",
      "Niveau de pr√©sence d'outliers : FAIBLE\n",
      "\n",
      "================================================================================\n",
      " Variable : SALAIRE\n",
      "================================================================================\n",
      "\n",
      "M√©thode IQR (Interquartile Range) :\n",
      "   ‚Ä¢ Q1 (25e percentile)     : 31593.00\n",
      "   ‚Ä¢ Q3 (75e percentile)     : 38353.00\n",
      "   ‚Ä¢ IQR (Q3 - Q1)           : 6760.00\n",
      "   ‚Ä¢ Limite inf√©rieure       : 21453.00\n",
      "   ‚Ä¢ Limite sup√©rieure       : 48493.00\n",
      "   ‚Ä¢ Nombre d'outliers       : 117 (0.62%)\n",
      "\n",
      "M√©thode Z-Score (threshold = 3) :\n",
      "   ‚Ä¢ Nombre d'outliers       : 46 (0.24%)\n",
      "\n",
      "Statistiques descriptives :\n",
      "   ‚Ä¢ Moyenne                 : 34966.88\n",
      "   ‚Ä¢ M√©diane                 : 34968.00\n",
      "   ‚Ä¢ √âcart-type              : 5005.21\n",
      "   ‚Ä¢ Min                     : 14128.00\n",
      "   ‚Ä¢ Max                     : 53977.00\n",
      "\n",
      "Niveau de pr√©sence d'outliers : FAIBLE\n",
      "\n",
      "================================================================================\n",
      " Variable : NOTE\n",
      "================================================================================\n",
      "\n",
      "M√©thode IQR (Interquartile Range) :\n",
      "   ‚Ä¢ Q1 (25e percentile)     : 63.73\n",
      "   ‚Ä¢ Q3 (75e percentile)     : 86.67\n",
      "   ‚Ä¢ IQR (Q3 - Q1)           : 22.94\n",
      "   ‚Ä¢ Limite inf√©rieure       : 29.32\n",
      "   ‚Ä¢ Limite sup√©rieure       : 121.08\n",
      "   ‚Ä¢ Nombre d'outliers       : 143 (0.75%)\n",
      "\n",
      "M√©thode Z-Score (threshold = 3) :\n",
      "   ‚Ä¢ Nombre d'outliers       : 48 (0.25%)\n",
      "\n",
      "Statistiques descriptives :\n",
      "   ‚Ä¢ Moyenne                 : 75.18\n",
      "   ‚Ä¢ M√©diane                 : 75.11\n",
      "   ‚Ä¢ √âcart-type              : 17.11\n",
      "   ‚Ä¢ Min                     : 8.68\n",
      "   ‚Ä¢ Max                     : 143.22\n",
      "\n",
      "Niveau de pr√©sence d'outliers : FAIBLE\n"
     ]
    }
   ],
   "source": [
    "# ============= ANALYSE DES OUTLIERS SUR df_dropna (R√âF√âRENCE) =============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFICATION DES OUTLIERS - Dataset de r√©f√©rence (df_dropna)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "outliers_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" Variable : {col.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # M√©thode IQR\n",
    "    iqr_results = detect_outliers_iqr(df_dropna, col)\n",
    "    \n",
    "    # M√©thode Z-score\n",
    "    zscore_results = detect_outliers_zscore(df_dropna, col, threshold=3)\n",
    "    \n",
    "    print(f\"\\nM√©thode IQR (Interquartile Range) :\")\n",
    "    print(f\"   ‚Ä¢ Q1 (25e percentile)     : {iqr_results['Q1']:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Q3 (75e percentile)     : {iqr_results['Q3']:.2f}\")\n",
    "    print(f\"   ‚Ä¢ IQR (Q3 - Q1)           : {iqr_results['IQR']:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Limite inf√©rieure       : {iqr_results['lower_bound']:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Limite sup√©rieure       : {iqr_results['upper_bound']:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Nombre d'outliers       : {iqr_results['n_outliers']} ({iqr_results['pct_outliers']:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nM√©thode Z-Score (threshold = 3) :\")\n",
    "    print(f\"   ‚Ä¢ Nombre d'outliers       : {zscore_results['n_outliers']} ({zscore_results['pct_outliers']:.2f}%)\")\n",
    "    \n",
    "    # Statistiques descriptives\n",
    "    print(f\"\\nStatistiques descriptives :\")\n",
    "    print(f\"   ‚Ä¢ Moyenne                 : {df_dropna[col].mean():.2f}\")\n",
    "    print(f\"   ‚Ä¢ M√©diane                 : {df_dropna[col].median():.2f}\")\n",
    "    print(f\"   ‚Ä¢ √âcart-type              : {df_dropna[col].std():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Min                     : {df_dropna[col].min():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Max                     : {df_dropna[col].max():.2f}\")\n",
    "    \n",
    "    # √âvaluation de la s√©v√©rit√©\n",
    "    severity = \"FAIBLE\" if iqr_results['pct_outliers'] < 5 else \"MOD√âR√â\" if iqr_results['pct_outliers'] < 10 else \"√âLEV√â\"\n",
    "    print(f\"\\nNiveau de pr√©sence d'outliers : {severity}\")\n",
    "    \n",
    "    # Stocker les r√©sultats\n",
    "    outliers_summary.append({\n",
    "        'Variable': col,\n",
    "        'N_outliers_IQR': iqr_results['n_outliers'],\n",
    "        'Pct_outliers_IQR': iqr_results['pct_outliers'],\n",
    "        'N_outliers_Zscore': zscore_results['n_outliers'],\n",
    "        'Pct_outliers_Zscore': zscore_results['pct_outliers'],\n",
    "        'Lower_bound': iqr_results['lower_bound'],\n",
    "        'Upper_bound': iqr_results['upper_bound']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "334c7dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TABLEAU R√âCAPITULATIF DES OUTLIERS\n",
      "================================================================================\n",
      "\n",
      "Variable  N_outliers_IQR  Pct_outliers_IQR  N_outliers_Zscore  Pct_outliers_Zscore  Lower_bound  Upper_bound\n",
      "   index               0          0.000000                  0             0.000000    -10024.50     30003.50\n",
      "     age             203          1.067241                 61             0.320698        11.00        59.00\n",
      "     exp               6          0.031544                 45             0.236581        -0.50        19.50\n",
      " salaire             117          0.615110                 46             0.241838     21453.00     48493.00\n",
      "    note             143          0.751801                 48             0.252353        29.32       121.08\n"
     ]
    }
   ],
   "source": [
    "# ============= TABLEAU R√âCAPITULATIF =============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLEAU R√âCAPITULATIF DES OUTLIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "outliers_df = pd.DataFrame(outliers_summary)\n",
    "print(\"\\n\" + outliers_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c41034",
   "metadata": {},
   "source": [
    "\n",
    "Globalement le taux d'outliers est faible pour toutes les varaibles donc on laisse comme √ßa.\n",
    "Cependant pour les notes >100 on va faire des imputations puisque la note maximale c'est 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d7c4be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FONCTION D'IMPUTATION PAR PLUS PROCHE VOISIN\n",
    "# =============================================================================\n",
    "\n",
    "def imputer_notes_knn(df_input, nom_base):\n",
    "    \"\"\"\n",
    "    Impute les notes > 100 en utilisant la note du plus proche voisin valide\n",
    "    \n",
    "    Param√®tres :\n",
    "    - df_input : DataFrame √† traiter\n",
    "    - nom_base : nom du fichier pour l'affichage\n",
    "    \n",
    "    Retourne :\n",
    "    - DataFrame avec notes imput√©es\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Traitement de : {nom_base}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Copie pour ne pas modifier l'original\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # Identifier les notes > 100\n",
    "    mask_outliers = df['note'] > 100\n",
    "    nb_outliers = mask_outliers.sum()\n",
    "    \n",
    "    print(f\"\\nD√©tection :\")\n",
    "    print(f\"Notes > 100 : {nb_outliers}\")\n",
    "    \n",
    "    if nb_outliers == 0:\n",
    "        print(\"Aucune note > 100 d√©tect√©e\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Pourcentage : {100*nb_outliers/len(df):.2f}%\")\n",
    "    \n",
    "    \n",
    "    # Pr√©parer les donn√©es pour KNN\n",
    "    # Encoder le dipl√¥me\n",
    "    le = LabelEncoder()\n",
    "    df['diplome_encoded'] = le.fit_transform(df['diplome'])\n",
    "    \n",
    "    # Variables pour calculer la similarit√©\n",
    "    features = ['age', 'exp', 'salaire', 'diplome_encoded']\n",
    "    \n",
    "    # S√©parer les donn√©es valides (note ‚â§ 100) et invalides (note > 100)\n",
    "    df_valides = df[df['note'] <= 100].copy()\n",
    "    df_invalides = df[df['note'] > 100].copy()\n",
    "    \n",
    "    print(f\"\\nImputation par plus proche voisin :\")\n",
    "    print(f\"Candidats valides (note ‚â§ 100) : {len(df_valides)}\")\n",
    "    print(f\"Candidats √† imputer (note > 100) : {len(df_invalides)}\")\n",
    "    \n",
    "    # Cr√©er le mod√®le KNN\n",
    "    knn = NearestNeighbors(n_neighbors=1, metric='euclidean')\n",
    "    knn.fit(df_valides[features])\n",
    "    \n",
    "    # Pour chaque note > 100, trouver le plus proche voisin\n",
    "    corrections = []\n",
    "    \n",
    "    for idx in df_invalides.index:\n",
    "        note_originale = df.loc[idx, 'note']\n",
    "        \n",
    "        # Caract√©ristiques du candidat\n",
    "        candidat_features = df.loc[idx, features].values.reshape(1, -1)\n",
    "        \n",
    "        # Trouver le plus proche voisin\n",
    "        distance, indice = knn.kneighbors(candidat_features)\n",
    "        voisin_idx = df_valides.iloc[indice[0][0]].name\n",
    "        note_voisin = df_valides.loc[voisin_idx, 'note']\n",
    "        \n",
    "        # Imputer\n",
    "        df.loc[idx, 'note'] = note_voisin\n",
    "        \n",
    "        corrections.append({\n",
    "            'index': idx,\n",
    "            'note_originale': note_originale,\n",
    "            'note_imputee': note_voisin,\n",
    "            'distance': distance[0][0],\n",
    "            'age': df.loc[idx, 'age'],\n",
    "            'exp': df.loc[idx, 'exp'],\n",
    "            'diplome': df.loc[idx, 'diplome']\n",
    "        })\n",
    "    \n",
    "    # Afficher les corrections\n",
    "    print(f\"\\n{len(corrections)} notes imput√©es\")\n",
    "    \n",
    "    # Supprimer la colonne temporaire\n",
    "    df.drop('diplome_encoded', axis=1, inplace=True)\n",
    "    \n",
    "    # V√©rification finale\n",
    "    nb_outliers_final = (df['note'] > 100).sum()\n",
    "    print(f\"\\nV√©rification finale :\")\n",
    "    print(f\"   Notes > 100 restantes : {nb_outliers_final}\")\n",
    "    \n",
    "    if nb_outliers_final == 0:\n",
    "        print(f\"SUCC√àS : Toutes les notes sont maintenant ‚â§ 100\")\n",
    "    \n",
    "    # Statistiques apr√®s correction\n",
    "    print(f\"\\nStatistiques de NOTE apr√®s correction :\")\n",
    "    print(f\"   ‚Ä¢ Min : {df['note'].min():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Max : {df['note'].max():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Moyenne : {df['note'].mean():.2f}\")\n",
    "    print(f\"   ‚Ä¢ M√©diane : {df['note'].median():.2f}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3ac8b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLICATION SUR LES 3 BASES DE DONN√âES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_dropna.csv\n",
      "================================================================================\n",
      "\n",
      "D√©tection :\n",
      "Notes > 100 : 1401\n",
      "Pourcentage : 7.37%\n",
      "\n",
      "Imputation par plus proche voisin :\n",
      "Candidats valides (note ‚â§ 100) : 17620\n",
      "Candidats √† imputer (note > 100) : 1401\n",
      "\n",
      "1401 notes imput√©es\n",
      "\n",
      "V√©rification finale :\n",
      "   Notes > 100 restantes : 0\n",
      "SUCC√àS : Toutes les notes sont maintenant ‚â§ 100\n",
      "\n",
      "Statistiques de NOTE apr√®s correction :\n",
      "   ‚Ä¢ Min : 8.68\n",
      "   ‚Ä¢ Max : 100.00\n",
      "   ‚Ä¢ Moyenne : 72.97\n",
      "   ‚Ä¢ M√©diane : 74.00\n",
      "\n",
      "   Base sauvegard√©e : ../data\\df_dropna.csv\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_median_mode.csv\n",
      "================================================================================\n",
      "\n",
      "D√©tection :\n",
      "Notes > 100 : 1465\n",
      "Pourcentage : 7.33%\n",
      "\n",
      "Imputation par plus proche voisin :\n",
      "Candidats valides (note ‚â§ 100) : 18535\n",
      "Candidats √† imputer (note > 100) : 1465\n",
      "\n",
      "1465 notes imput√©es\n",
      "\n",
      "V√©rification finale :\n",
      "   Notes > 100 restantes : 0\n",
      "SUCC√àS : Toutes les notes sont maintenant ‚â§ 100\n",
      "\n",
      "Statistiques de NOTE apr√®s correction :\n",
      "   ‚Ä¢ Min : 8.68\n",
      "   ‚Ä¢ Max : 100.00\n",
      "   ‚Ä¢ Moyenne : 72.97\n",
      "   ‚Ä¢ M√©diane : 74.06\n",
      "\n",
      "   Base sauvegard√©e : ../data\\df_median_mode.csv\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_knn.csv\n",
      "================================================================================\n",
      "\n",
      "D√©tection :\n",
      "Notes > 100 : 1465\n",
      "Pourcentage : 7.33%\n",
      "\n",
      "Imputation par plus proche voisin :\n",
      "Candidats valides (note ‚â§ 100) : 18535\n",
      "Candidats √† imputer (note > 100) : 1465\n",
      "\n",
      "1465 notes imput√©es\n",
      "\n",
      "V√©rification finale :\n",
      "   Notes > 100 restantes : 0\n",
      "SUCC√àS : Toutes les notes sont maintenant ‚â§ 100\n",
      "\n",
      "Statistiques de NOTE apr√®s correction :\n",
      "   ‚Ä¢ Min : 8.68\n",
      "   ‚Ä¢ Max : 100.00\n",
      "   ‚Ä¢ Moyenne : 72.96\n",
      "   ‚Ä¢ M√©diane : 73.98\n",
      "\n",
      "   Base sauvegard√©e : ../data\\df_knn.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLICATION SUR LES 3 BASES DE DONN√âES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLICATION SUR LES 3 BASES DE DONN√âES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Chemin du dossier data\n",
    "data_path = '../data'\n",
    "\n",
    "bases = [\n",
    "    'df_dropna.csv',\n",
    "    'df_median_mode.csv', \n",
    "    'df_knn.csv'\n",
    "]\n",
    "\n",
    "for nom_base in bases:\n",
    "    try:\n",
    "        # Chemin complet\n",
    "        chemin_fichier = os.path.join(data_path, nom_base)\n",
    "        \n",
    "        # Charger la base\n",
    "        df = pd.read_csv(chemin_fichier)\n",
    "        \n",
    "        # Appliquer l'imputation\n",
    "        df_corrige = imputer_notes_knn(df, nom_base)\n",
    "        \n",
    "        # Sauvegarder\n",
    "        df_corrige.to_csv(chemin_fichier, index=False)\n",
    "        print(f\"\\n   Base sauvegard√©e : {chemin_fichier}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n   ATTENTION : Fichier non trouv√© : {chemin_fichier}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\n   ERREUR lors du traitement de {nom_base} : {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01994b",
   "metadata": {},
   "source": [
    "#### SECTION 2 : FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54917f4e",
   "metadata": {},
   "source": [
    "Objectif : Cr√©er de nouvelles variables pour am√©liorer le pouvoir pr√©dictif\n",
    "Strat√©gie : \n",
    "- Ratios et variables d√©riv√©es (exp/age, salaire/exp, etc.)\n",
    "- Variables temporelles (mois, trimestre, ann√©e)\n",
    "- Cat√©gorisation de variables continues\n",
    "- Interactions entre variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c4656401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FONCTION DE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "def creer_features(df_input, nom_base):\n",
    "    \"\"\"\n",
    "    Cr√©e de nouvelles features √† partir des variables existantes\n",
    "    \n",
    "    Param√®tres :\n",
    "    - df_input : DataFrame √† enrichir\n",
    "    - nom_base : nom du fichier pour l'affichage\n",
    "    \n",
    "    Retourne :\n",
    "    - DataFrame enrichi avec nouvelles features\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Traitement de : {nom_base}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = df_input.copy()\n",
    "    \n",
    "    print(f\"\\nNombre de colonnes AVANT : {len(df.columns)}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. VARIABLES TEMPORELLES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[1/5] Extraction des variables temporelles...\")\n",
    "    \n",
    "    # Convertir la colonne date si n√©cessaire\n",
    "    if df['date'].dtype == 'object':\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Extraire les composantes temporelles\n",
    "    df['annee'] = df['date'].dt.year\n",
    "    df['mois'] = df['date'].dt.month\n",
    "    df['trimestre'] = df['date'].dt.quarter\n",
    "    df['jour_semaine'] = df['date'].dt.dayofweek  # 0=Lundi, 6=Dimanche\n",
    "    df['semaine_annee'] = df['date'].dt.isocalendar().week\n",
    "    \n",
    "    print(f\"   - 5 variables temporelles cr√©√©es (annee, mois, trimestre, jour_semaine, semaine_annee)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. RATIOS ET VARIABLES D√âRIV√âES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[2/5] Cr√©ation de ratios et variables d√©riv√©es...\")\n",
    "    \n",
    "    # Ratio exp√©rience / √¢ge (exp√©rience relative)\n",
    "    df['exp_age_ratio'] = df['exp'] / (df['age'] - 18)\n",
    "    df['exp_age_ratio'] = df['exp_age_ratio'].replace([np.inf, -np.inf], 0)  # G√©rer division par z√©ro\n",
    "    \n",
    "    # Salaire par ann√©e d'exp√©rience\n",
    "    df['salaire_par_exp'] = df['salaire'] / (df['exp'] + 1)  # +1 pour √©viter division par z√©ro\n",
    "    \n",
    "    # Diff√©rence entre exp√©rience r√©elle et exp√©rience maximale possible\n",
    "    df['ecart_exp_max'] = (df['age'] - 18) - df['exp']\n",
    "    \n",
    "    print(f\"   - 4 ratios cr√©√©s (exp_age_ratio, salaire_par_exp, ecart_exp_max)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. CAT√âGORISATION DE VARIABLES CONTINUES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[3/5] Cat√©gorisation des variables continues...\")\n",
    "    \n",
    "    # Cat√©gories d'√¢ge\n",
    "    df['categorie_age'] = pd.cut(df['age'], \n",
    "                                  bins=[0, 25, 35, 45, 100], \n",
    "                                  labels=['Jeune', 'Junior', 'Confirme', 'Senior'])\n",
    "    \n",
    "    # Niveaux d'exp√©rience\n",
    "    df['niveau_experience'] = pd.cut(df['exp'], \n",
    "                                      bins=[-1, 2, 5, 10, 100], \n",
    "                                      labels=['Debutant', 'Intermediaire', 'Experimente', 'Expert'])\n",
    "    \n",
    "    # Tranches de salaire\n",
    "    df['tranche_salaire'] = pd.cut(df['salaire'], \n",
    "                                    bins=[0, 30000, 35000, 40000, 100000], \n",
    "                                    labels=['Bas', 'Moyen', 'Eleve', 'Tres_eleve'])\n",
    "    \n",
    "    # Niveaux de note\n",
    "    df['niveau_note'] = pd.cut(df['note'], \n",
    "                                bins=[0, 50, 70, 85, 100], \n",
    "                                labels=['Faible', 'Moyen', 'Bon', 'Excellent'])\n",
    "    \n",
    "    print(f\"   - 4 cat√©gorisations cr√©√©es (categorie_age, niveau_experience, tranche_salaire, niveau_note)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. INTERACTIONS ENTRE VARIABLES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[4/5] Cr√©ation d'interactions entre variables...\")\n",
    "    \n",
    "    # Encoder le dipl√¥me en num√©rique pour les interactions\n",
    "    diplome_mapping = {'bac': 1, 'licence': 2, 'master': 3, 'doctorat': 4}\n",
    "    df['diplome_num'] = df['diplome'].map(diplome_mapping)\n",
    "    \n",
    "    # Interaction dipl√¥me √ó exp√©rience\n",
    "    df['diplome_x_exp'] = df['diplome_num'] * df['exp']\n",
    "    \n",
    "    # Interaction dipl√¥me √ó note\n",
    "    df['diplome_x_note'] = df['diplome_num'] * df['note']\n",
    "    \n",
    "    # Interaction exp√©rience √ó note\n",
    "    df['exp_x_note'] = df['exp'] * df['note']\n",
    "    \n",
    "    # Score composite : (dipl√¥me + exp + note normalis√©e)\n",
    "    df['score_composite'] = (\n",
    "        df['diplome_num'] * 10 + \n",
    "        df['exp'] + \n",
    "        df['note'] / 10\n",
    "    )\n",
    "    \n",
    "    print(f\"   - 5 interactions cr√©√©es (diplome_x_exp, diplome_x_note, exp_x_note, score_composite, diplome_num)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5. VARIABLES BINAIRES ET FLAGS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[5/5] Cr√©ation de variables binaires et flags...\")\n",
    "    \n",
    "    # Flag : exp√©rience √©lev√©e pour l'√¢ge\n",
    "    df['exp_elevee_pour_age'] = (df['exp_age_ratio'] > df['exp_age_ratio'].median()).astype(int)\n",
    "    \n",
    "    # Flag : salaire √©lev√©\n",
    "    df['salaire_eleve'] = (df['salaire'] > df['salaire'].median()).astype(int)\n",
    "    \n",
    "    # Flag : note √©lev√©e\n",
    "    df['note_elevee'] = (df['note'] > df['note'].median()).astype(int)\n",
    "    \n",
    "    # Flag : dipl√¥me √©lev√© (master ou doctorat)\n",
    "    df['diplome_eleve'] = df['diplome'].isin(['master', 'doctorat']).astype(int)\n",
    "    \n",
    "    # Flag : senior (√¢ge > 40 et exp > 10)\n",
    "    df['est_senior'] = ((df['age'] > 40) & (df['exp'] > 10)).astype(int)\n",
    "    \n",
    "    # Flag : profil atypique (doctorat mais salaire bas)\n",
    "    df['profil_atypique'] = ((df['diplome'] == 'doctorat') & (df['salaire'] < 30000)).astype(int)\n",
    "    \n",
    "    print(f\"   - 6 flags binaires cr√©√©s (exp_elevee_pour_age, salaire_eleve, note_elevee, diplome_eleve, est_senior, profil_atypique)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # R√âCAPITULATIF\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"R√âCAPITULATIF - {nom_base}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    nb_nouvelles_features = len(df.columns) - len(df_input.columns)\n",
    "    \n",
    "    print(f\"\\nNombre de colonnes APR√àS : {len(df.columns)}\")\n",
    "    print(f\"Nouvelles features cr√©√©es : {nb_nouvelles_features}\")\n",
    "    \n",
    "    print(f\"\\nCat√©gories de features cr√©√©es :\")\n",
    "    print(f\"   - Variables temporelles : 5\")\n",
    "    print(f\"   - Ratios et d√©riv√©es : 4\")\n",
    "    print(f\"   - Cat√©gorisations : 4\")\n",
    "    print(f\"   - Interactions : 5\")\n",
    "    print(f\"   - Flags binaires : 6\")\n",
    "    print(f\"   TOTAL : 24 nouvelles features\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3f36f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLICATION SUR LES 3 BASES DE DONN√âES\n",
      "================================================================================\n",
      "\n",
      "Chargement de df_dropna.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_dropna.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes AVANT : 12\n",
      "\n",
      "[1/5] Extraction des variables temporelles...\n",
      "   - 5 variables temporelles cr√©√©es (annee, mois, trimestre, jour_semaine, semaine_annee)\n",
      "\n",
      "[2/5] Cr√©ation de ratios et variables d√©riv√©es...\n",
      "   - 4 ratios cr√©√©s (exp_age_ratio, salaire_par_exp, ecart_exp_max)\n",
      "\n",
      "[3/5] Cat√©gorisation des variables continues...\n",
      "   - 4 cat√©gorisations cr√©√©es (categorie_age, niveau_experience, tranche_salaire, niveau_note)\n",
      "\n",
      "[4/5] Cr√©ation d'interactions entre variables...\n",
      "   - 5 interactions cr√©√©es (diplome_x_exp, diplome_x_note, exp_x_note, score_composite, diplome_num)\n",
      "\n",
      "[5/5] Cr√©ation de variables binaires et flags...\n",
      "   - 6 flags binaires cr√©√©s (exp_elevee_pour_age, salaire_eleve, note_elevee, diplome_eleve, est_senior, profil_atypique)\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_dropna.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes APR√àS : 35\n",
      "Nouvelles features cr√©√©es : 23\n",
      "\n",
      "Cat√©gories de features cr√©√©es :\n",
      "   - Variables temporelles : 5\n",
      "   - Ratios et d√©riv√©es : 4\n",
      "   - Cat√©gorisations : 4\n",
      "   - Interactions : 5\n",
      "   - Flags binaires : 6\n",
      "   TOTAL : 24 nouvelles features\n",
      "\n",
      "Base enrichie sauvegard√©e : ../data\\df_dropna_features.csv\n",
      "\n",
      "Chargement de df_median_mode.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_median_mode.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes AVANT : 12\n",
      "\n",
      "[1/5] Extraction des variables temporelles...\n",
      "   - 5 variables temporelles cr√©√©es (annee, mois, trimestre, jour_semaine, semaine_annee)\n",
      "\n",
      "[2/5] Cr√©ation de ratios et variables d√©riv√©es...\n",
      "   - 4 ratios cr√©√©s (exp_age_ratio, salaire_par_exp, ecart_exp_max)\n",
      "\n",
      "[3/5] Cat√©gorisation des variables continues...\n",
      "   - 4 cat√©gorisations cr√©√©es (categorie_age, niveau_experience, tranche_salaire, niveau_note)\n",
      "\n",
      "[4/5] Cr√©ation d'interactions entre variables...\n",
      "   - 5 interactions cr√©√©es (diplome_x_exp, diplome_x_note, exp_x_note, score_composite, diplome_num)\n",
      "\n",
      "[5/5] Cr√©ation de variables binaires et flags...\n",
      "   - 6 flags binaires cr√©√©s (exp_elevee_pour_age, salaire_eleve, note_elevee, diplome_eleve, est_senior, profil_atypique)\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_median_mode.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes APR√àS : 35\n",
      "Nouvelles features cr√©√©es : 23\n",
      "\n",
      "Cat√©gories de features cr√©√©es :\n",
      "   - Variables temporelles : 5\n",
      "   - Ratios et d√©riv√©es : 4\n",
      "   - Cat√©gorisations : 4\n",
      "   - Interactions : 5\n",
      "   - Flags binaires : 6\n",
      "   TOTAL : 24 nouvelles features\n",
      "\n",
      "Base enrichie sauvegard√©e : ../data\\df_median_mode_features.csv\n",
      "\n",
      "Chargement de df_knn.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_knn.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes AVANT : 12\n",
      "\n",
      "[1/5] Extraction des variables temporelles...\n",
      "   - 5 variables temporelles cr√©√©es (annee, mois, trimestre, jour_semaine, semaine_annee)\n",
      "\n",
      "[2/5] Cr√©ation de ratios et variables d√©riv√©es...\n",
      "   - 4 ratios cr√©√©s (exp_age_ratio, salaire_par_exp, ecart_exp_max)\n",
      "\n",
      "[3/5] Cat√©gorisation des variables continues...\n",
      "   - 4 cat√©gorisations cr√©√©es (categorie_age, niveau_experience, tranche_salaire, niveau_note)\n",
      "\n",
      "[4/5] Cr√©ation d'interactions entre variables...\n",
      "   - 5 interactions cr√©√©es (diplome_x_exp, diplome_x_note, exp_x_note, score_composite, diplome_num)\n",
      "\n",
      "[5/5] Cr√©ation de variables binaires et flags...\n",
      "   - 6 flags binaires cr√©√©s (exp_elevee_pour_age, salaire_eleve, note_elevee, diplome_eleve, est_senior, profil_atypique)\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_knn.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes APR√àS : 35\n",
      "Nouvelles features cr√©√©es : 23\n",
      "\n",
      "Cat√©gories de features cr√©√©es :\n",
      "   - Variables temporelles : 5\n",
      "   - Ratios et d√©riv√©es : 4\n",
      "   - Cat√©gorisations : 4\n",
      "   - Interactions : 5\n",
      "   - Flags binaires : 6\n",
      "   TOTAL : 24 nouvelles features\n",
      "\n",
      "Base enrichie sauvegard√©e : ../data\\df_knn_features.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# APPLICATION SUR LES 3 BASES DE DONN√âES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLICATION SUR LES 3 BASES DE DONN√âES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Chemin du dossier data\n",
    "data_path = '../data'\n",
    "\n",
    "bases = [\n",
    "    'df_dropna.csv',\n",
    "    'df_median_mode.csv', \n",
    "    'df_knn.csv'\n",
    "]\n",
    "\n",
    "for nom_base in bases:\n",
    "    try:\n",
    "        # Chemin complet\n",
    "        chemin_fichier = os.path.join(data_path, nom_base)\n",
    "        \n",
    "        # Charger la base\n",
    "        print(f\"\\nChargement de {nom_base}...\")\n",
    "        df = pd.read_csv(chemin_fichier)\n",
    "        \n",
    "        # Appliquer le feature engineering\n",
    "        df_enrichi = creer_features(df, nom_base)\n",
    "        \n",
    "        # Sauvegarder avec un nouveau nom pour garder trace\n",
    "        nom_output = nom_base.replace('.csv', '_features.csv')\n",
    "        chemin_output = os.path.join(data_path, nom_output)\n",
    "        df_enrichi.to_csv(chemin_output, index=False)\n",
    "        \n",
    "        print(f\"\\nBase enrichie sauvegard√©e : {chemin_output}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nATTENTION : Fichier non trouv√© : {chemin_fichier}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERREUR lors du traitement de {nom_base} : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c15cc2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "V√âRIFICATION DES NOUVELLES FEATURES\n",
      "================================================================================\n",
      "\n",
      "Aper√ßu des nouvelles features (df_median_mode_features.csv) :\n",
      "\n",
      "Colonnes ajout√©es :\n",
      "   1. annee\n",
      "   2. mois\n",
      "   3. trimestre\n",
      "   4. jour_semaine\n",
      "   5. semaine_annee\n",
      "   6. exp_age_ratio\n",
      "   7. salaire_par_exp\n",
      "   8. ecart_exp_max\n",
      "   9. categorie_age\n",
      "   10. niveau_experience\n",
      "   11. tranche_salaire\n",
      "   12. niveau_note\n",
      "   13. diplome_num\n",
      "   14. diplome_x_exp\n",
      "   15. diplome_x_note\n",
      "   16. exp_x_note\n",
      "   17. score_composite\n",
      "   18. exp_elevee_pour_age\n",
      "   19. salaire_eleve\n",
      "   20. note_elevee\n",
      "   21. diplome_eleve\n",
      "   22. est_senior\n",
      "   23. profil_atypique\n",
      "\n",
      "Statistiques descriptives des nouvelles variables num√©riques :\n",
      "          annee      mois  trimestre  jour_semaine  semaine_annee  \\\n",
      "count  20000.00  20000.00   20000.00      20000.00       20000.00   \n",
      "mean    2012.00      6.51       2.50          3.01          26.45   \n",
      "std        1.41      3.46       1.12          2.00          15.13   \n",
      "min     2010.00      1.00       1.00          0.00           1.00   \n",
      "25%     2011.00      4.00       2.00          1.00          13.00   \n",
      "50%     2012.00      7.00       3.00          3.00          26.00   \n",
      "75%     2013.00     10.00       4.00          5.00          40.00   \n",
      "max     2014.00     12.00       4.00          6.00          53.00   \n",
      "\n",
      "       exp_age_ratio  salaire_par_exp  ecart_exp_max  diplome_num  \\\n",
      "count       20000.00         20000.00       20000.00     20000.00   \n",
      "mean            0.67          3726.64           7.51         2.51   \n",
      "std             1.58          1914.27          10.04         0.87   \n",
      "min           -16.00          1025.45         -32.00         1.00   \n",
      "25%             0.35          2717.90           1.00         2.00   \n",
      "50%             0.53          3339.47           7.00         3.00   \n",
      "75%             0.82          4197.47          14.00         3.00   \n",
      "max            17.00         41646.00          53.00         4.00   \n",
      "\n",
      "       diplome_x_exp  diplome_x_note  exp_x_note  score_composite  \\\n",
      "count       20000.00        20000.00    20000.00         20000.00   \n",
      "mean           23.87          189.23      692.30            41.93   \n",
      "std            11.44           85.93      262.42             9.89   \n",
      "min             0.00            8.68        0.00            14.86   \n",
      "25%            15.00          128.38      503.30            35.15   \n",
      "50%            22.00          179.09      675.50            42.21   \n",
      "75%            30.00          249.54      858.44            48.68   \n",
      "max            76.00          399.56     1833.50            68.65   \n",
      "\n",
      "       exp_elevee_pour_age  salaire_eleve  note_elevee  diplome_eleve  \\\n",
      "count              20000.0        20000.0      20000.0       20000.00   \n",
      "mean                   0.5            0.5          0.5           0.51   \n",
      "std                    0.5            0.5          0.5           0.50   \n",
      "min                    0.0            0.0          0.0           0.00   \n",
      "25%                    0.0            0.0          0.0           0.00   \n",
      "50%                    0.0            0.0          0.0           1.00   \n",
      "75%                    1.0            1.0          1.0           1.00   \n",
      "max                    1.0            1.0          1.0           1.00   \n",
      "\n",
      "       est_senior  profil_atypique  \n",
      "count     20000.0         20000.00  \n",
      "mean          0.1             0.05  \n",
      "std           0.3             0.21  \n",
      "min           0.0             0.00  \n",
      "25%           0.0             0.00  \n",
      "50%           0.0             0.00  \n",
      "75%           0.0             0.00  \n",
      "max           1.0             1.00  \n",
      "\n",
      "Exemples de valeurs :\n",
      "    age   exp  exp_age_ratio categorie_age niveau_experience  score_composite  \\\n",
      "0  25.0   9.0       1.285714         Jeune       Experimente           38.708   \n",
      "1  35.0  13.0       0.764706        Junior            Expert           39.386   \n",
      "2  29.0  13.0       1.181818        Junior            Expert           40.850   \n",
      "3  35.0  12.0       0.705882        Junior            Expert           36.509   \n",
      "4  35.0   6.0       0.352941        Junior       Experimente           34.191   \n",
      "5  37.0   8.0       0.421053      Confirme       Experimente           44.346   \n",
      "6  33.0  12.0       0.800000        Junior            Expert           47.020   \n",
      "7  31.0  10.0       0.769231        Junior       Experimente           36.220   \n",
      "8  43.0  10.0       0.400000      Confirme       Experimente           56.517   \n",
      "9  28.0  11.0       1.100000        Junior            Expert           47.693   \n",
      "\n",
      "   est_senior  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "5           0  \n",
      "6           0  \n",
      "7           0  \n",
      "8           0  \n",
      "9           0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# V√âRIFICATION DES NOUVELLES FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"V√âRIFICATION DES NOUVELLES FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Charger une des bases enrichies pour v√©rification\n",
    "try:\n",
    "    chemin_verif = os.path.join(data_path, 'df_median_mode_features.csv')\n",
    "    df_verif = pd.read_csv(chemin_verif)\n",
    "    \n",
    "    print(f\"\\nAper√ßu des nouvelles features (df_median_mode_features.csv) :\")\n",
    "    print(f\"\\nColonnes ajout√©es :\")\n",
    "    \n",
    "    colonnes_originales = ['index', 'date', 'cheveux', 'age', 'exp', 'salaire', \n",
    "                          'sexe', 'diplome', 'specialite', 'note', 'dispo', 'embauche']\n",
    "    nouvelles_colonnes = [col for col in df_verif.columns if col not in colonnes_originales]\n",
    "    \n",
    "    for i, col in enumerate(nouvelles_colonnes, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "    \n",
    "    print(f\"\\nStatistiques descriptives des nouvelles variables num√©riques :\")\n",
    "    nouvelles_num = df_verif[nouvelles_colonnes].select_dtypes(include=[np.number])\n",
    "    print(nouvelles_num.describe().round(2))\n",
    "    \n",
    "    print(f\"\\nExemples de valeurs :\")\n",
    "    print(df_verif[['age', 'exp', 'exp_age_ratio', 'categorie_age', 'niveau_experience', \n",
    "                   'score_composite', 'est_senior']].head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nImpossible de charger le fichier de v√©rification : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ceb31440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BILAN FINAL DU FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "Feature Engineering termin√© avec succ√®s !\n",
      "\n",
      "   - 24 nouvelles features cr√©√©es par base\n",
      "   - 3 bases enrichies sauvegard√©es avec suffixe '_features.csv'\n",
      "\n",
      "Fichiers g√©n√©r√©s :\n",
      "   - ../data/df_dropna_features.csv\n",
      "   - ../data/df_median_mode_features.csv\n",
      "   - ../data/df_knn_features.csv\n",
      "\n",
      "Cat√©gories de features :\n",
      "   [1] Temporelles (5) : mois, trimestre, ann√©e, jour_semaine, semaine_annee\n",
      "   [2] Ratios (4) : exp_age_ratio, salaire_par_exp, etc.\n",
      "   [3] Cat√©gories (4) : categorie_age, niveau_experience, etc.\n",
      "   [4] Interactions (5) : diplome_x_exp, diplome_x_note, etc.\n",
      "   [5] Flags (6) : est_senior, salaire_eleve, etc.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BILAN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BILAN FINAL DU FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFeature Engineering termin√© avec succ√®s !\\n\")\n",
    "print(f\"   - 24 nouvelles features cr√©√©es par base\")\n",
    "print(f\"   - 3 bases enrichies sauvegard√©es avec suffixe '_features.csv'\")\n",
    "print(f\"\\nFichiers g√©n√©r√©s :\")\n",
    "print(f\"   - ../data/df_dropna_features.csv\")\n",
    "print(f\"   - ../data/df_median_mode_features.csv\")\n",
    "print(f\"   - ../data/df_knn_features.csv\")\n",
    "\n",
    "print(f\"\\nCat√©gories de features :\")\n",
    "print(f\"   [1] Temporelles (5) : mois, trimestre, ann√©e, jour_semaine, semaine_annee\")\n",
    "print(f\"   [2] Ratios (4) : exp_age_ratio, salaire_par_exp, etc.\")\n",
    "print(f\"   [3] Cat√©gories (4) : categorie_age, niveau_experience, etc.\")\n",
    "print(f\"   [4] Interactions (5) : diplome_x_exp, diplome_x_note, etc.\")\n",
    "print(f\"   [5] Flags (6) : est_senior, salaire_eleve, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a3553",
   "metadata": {},
   "source": [
    "#### SECTION 3 : ENCODAGE DES VARIABLES CATEGORIELLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8f135",
   "metadata": {},
   "source": [
    "Objectif : Transformer les variables cat√©gorielles en format num√©rique\n",
    "Strat√©gie : \n",
    "- Label Encoding pour variables ordinales et binaires (dipl√¥me, sexe, dispo)\n",
    "- One-Hot Encoding pour variables nominales (cheveux, sp√©cialit√©)\n",
    "- Encodage cyclique pour variables temporelles (mois, jour_semaine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a475981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FONCTION D'ENCODAGE\n",
    "# =============================================================================\n",
    "\n",
    "def encoder_variables(df_input, nom_base):\n",
    "    \"\"\"\n",
    "    Encode toutes les variables cat√©gorielles selon leur type\n",
    "    \n",
    "    Param√®tres :\n",
    "    - df_input : DataFrame √† encoder\n",
    "    - nom_base : nom du fichier pour l'affichage\n",
    "    \n",
    "    Retourne :\n",
    "    - DataFrame avec variables encod√©es\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Traitement de : {nom_base}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = df_input.copy()\n",
    "    \n",
    "    print(f\"\\nNombre de colonnes AVANT encodage : {len(df.columns)}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. LABEL ENCODING - VARIABLES ORDINALES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[1/4] Label Encoding pour variables ORDINALES...\")\n",
    "    \n",
    "    # DIPL√îME (hi√©rarchie claire : bac < licence < master < doctorat)\n",
    "    diplome_mapping = {\n",
    "        'bac': 0,\n",
    "        'licence': 1,\n",
    "        'master': 2,\n",
    "        'doctorat': 3\n",
    "    }\n",
    "    \n",
    "    if 'diplome' in df.columns:\n",
    "        df['diplome_encoded'] = df['diplome'].map(diplome_mapping)\n",
    "        print(f\"   - diplome : encod√© selon hi√©rarchie (bac=0, licence=1, master=2, doctorat=3)\")\n",
    "        print(f\"     R√©partition : {df['diplome'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # NIVEAU_EXPERIENCE (si cr√©√© en Feature Engineering)\n",
    "    if 'niveau_experience' in df.columns:\n",
    "        niveau_exp_mapping = {\n",
    "            'Debutant': 0,\n",
    "            'Intermediaire': 1,\n",
    "            'Experimente': 2,\n",
    "            'Expert': 3\n",
    "        }\n",
    "        df['niveau_experience_encoded'] = df['niveau_experience'].map(niveau_exp_mapping)\n",
    "        print(f\"   - niveau_experience : encod√© selon hi√©rarchie (Debutant=0, Expert=3)\")\n",
    "    \n",
    "    # CATEGORIE_AGE (si cr√©√©e en Feature Engineering)\n",
    "    if 'categorie_age' in df.columns:\n",
    "        categorie_age_mapping = {\n",
    "            'Jeune': 0,\n",
    "            'Junior': 1,\n",
    "            'Confirme': 2,\n",
    "            'Senior': 3\n",
    "        }\n",
    "        df['categorie_age_encoded'] = df['categorie_age'].map(categorie_age_mapping)\n",
    "        print(f\"   - categorie_age : encod√© selon hi√©rarchie (Jeune=0, Senior=3)\")\n",
    "    \n",
    "    # TRANCHE_SALAIRE (si cr√©√©e en Feature Engineering)\n",
    "    if 'tranche_salaire' in df.columns:\n",
    "        tranche_salaire_mapping = {\n",
    "            'Bas': 0,\n",
    "            'Moyen': 1,\n",
    "            'Eleve': 2,\n",
    "            'Tres_eleve': 3\n",
    "        }\n",
    "        df['tranche_salaire_encoded'] = df['tranche_salaire'].map(tranche_salaire_mapping)\n",
    "        print(f\"   - tranche_salaire : encod√© selon hi√©rarchie (Bas=0, Tres_eleve=3)\")\n",
    "    \n",
    "    # NIVEAU_NOTE (si cr√©√© en Feature Engineering)\n",
    "    if 'niveau_note' in df.columns:\n",
    "        niveau_note_mapping = {\n",
    "            'Faible': 0,\n",
    "            'Moyen': 1,\n",
    "            'Bon': 2,\n",
    "            'Excellent': 3\n",
    "        }\n",
    "        df['niveau_note_encoded'] = df['niveau_note'].map(niveau_note_mapping)\n",
    "        print(f\"   - niveau_note : encod√© selon hi√©rarchie (Faible=0, Excellent=3)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. LABEL ENCODING - VARIABLES BINAIRES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[2/4] Label Encoding pour variables BINAIRES...\")\n",
    "    \n",
    "    # SEXE\n",
    "    if 'sexe' in df.columns:\n",
    "        sexe_mapping = {'F': 0, 'M': 1}\n",
    "        df['sexe_encoded'] = df['sexe'].map(sexe_mapping)\n",
    "        print(f\"   - sexe : encod√© (F=0, M=1)\")\n",
    "        print(f\"     R√©partition : {df['sexe'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # DISPO\n",
    "    if 'dispo' in df.columns:\n",
    "        dispo_mapping = {'non': 0, 'oui': 1}\n",
    "        df['dispo_encoded'] = df['dispo'].map(dispo_mapping)\n",
    "        print(f\"   - dispo : encod√© (non=0, oui=1)\")\n",
    "        print(f\"     R√©partition : {df['dispo'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. ONE-HOT ENCODING - VARIABLES NOMINALES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[3/4] One-Hot Encoding pour variables NOMINALES...\")\n",
    "    \n",
    "    # CHEVEUX (pas de hi√©rarchie entre couleurs)\n",
    "    if 'cheveux' in df.columns:\n",
    "        nb_modalites = df['cheveux'].nunique()\n",
    "        df = pd.get_dummies(df, columns=['cheveux'], prefix='cheveux', drop_first=True)\n",
    "        print(f\"   - cheveux : {nb_modalites} modalit√©s -> {nb_modalites-1} colonnes binaires\")\n",
    "        print(f\"     Colonnes cr√©√©es : {[col for col in df.columns if col.startswith('cheveux_')]}\")\n",
    "    \n",
    "    # SPECIALITE (pas de hi√©rarchie entre sp√©cialit√©s)\n",
    "    if 'specialite' in df.columns:\n",
    "        nb_modalites = df['specialite'].nunique()\n",
    "        print(f\"   - specialite : {nb_modalites} modalit√©s d√©tect√©es\")\n",
    "        \n",
    "        # Si trop de modalit√©s (> 15), regrouper les rares en \"Autre\"\n",
    "        if nb_modalites > 15:\n",
    "            print(f\"     ATTENTION : Trop de modalit√©s ({nb_modalites})\")\n",
    "            print(f\"     Regroupement des sp√©cialit√©s rares en 'Autre'...\")\n",
    "            \n",
    "            # Garder les 15 sp√©cialit√©s les plus fr√©quentes\n",
    "            top_specialites = df['specialite'].value_counts().head(15).index.tolist()\n",
    "            df['specialite'] = df['specialite'].apply(\n",
    "                lambda x: x if x in top_specialites else 'Autre'\n",
    "            )\n",
    "            nb_modalites = df['specialite'].nunique()\n",
    "            print(f\"     Apr√®s regroupement : {nb_modalites} modalit√©s\")\n",
    "        \n",
    "        df = pd.get_dummies(df, columns=['specialite'], prefix='spec', drop_first=True)\n",
    "        print(f\"     Colonnes cr√©√©es : {nb_modalites-1}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. ENCODAGE CYCLIQUE - VARIABLES TEMPORELLES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[4/4] Encodage CYCLIQUE pour variables TEMPORELLES...\")\n",
    "    \n",
    "    # MOIS (cyclique : d√©cembre proche de janvier)\n",
    "    if 'mois' in df.columns:\n",
    "        df['mois_sin'] = np.sin(2 * np.pi * df['mois'] / 12)\n",
    "        df['mois_cos'] = np.cos(2 * np.pi * df['mois'] / 12)\n",
    "        print(f\"   - mois : encodage cyclique (mois_sin, mois_cos)\")\n",
    "        print(f\"     Justification : d√©cembre (12) est proche de janvier (1)\")\n",
    "    \n",
    "    # JOUR_SEMAINE (cyclique : dimanche proche de lundi)\n",
    "    if 'jour_semaine' in df.columns:\n",
    "        df['jour_semaine_sin'] = np.sin(2 * np.pi * df['jour_semaine'] / 7)\n",
    "        df['jour_semaine_cos'] = np.cos(2 * np.pi * df['jour_semaine'] / 7)\n",
    "        print(f\"   - jour_semaine : encodage cyclique (jour_semaine_sin, jour_semaine_cos)\")\n",
    "        print(f\"     Justification : dimanche (6) est proche de lundi (0)\")\n",
    "    \n",
    "    # TRIMESTRE (ordinal simple, pas besoin de cyclique)\n",
    "    if 'trimestre' in df.columns:\n",
    "        print(f\"   - trimestre : conserv√© tel quel (1, 2, 3, 4)\")\n",
    "        print(f\"     Note : d√©j√† num√©rique et ordinal\")\n",
    "    \n",
    "    # SEMAINE_ANNEE (cyclique : semaine 52 proche de semaine 1)\n",
    "    if 'semaine_annee' in df.columns:\n",
    "        df['semaine_annee_sin'] = np.sin(2 * np.pi * df['semaine_annee'] / 52)\n",
    "        df['semaine_annee_cos'] = np.cos(2 * np.pi * df['semaine_annee'] / 52)\n",
    "        print(f\"   - semaine_annee : encodage cyclique (semaine_annee_sin, semaine_annee_cos)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # NETTOYAGE - SUPPRESSION DES COLONNES ORIGINALES ENCOD√âES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[5/4] Nettoyage : suppression des colonnes originales encod√©es...\")\n",
    "    \n",
    "    colonnes_a_supprimer = []\n",
    "    \n",
    "    # Supprimer les versions textuelles si versions encod√©es existent\n",
    "    if 'diplome_encoded' in df.columns:\n",
    "        colonnes_a_supprimer.append('diplome')\n",
    "    if 'sexe_encoded' in df.columns:\n",
    "        colonnes_a_supprimer.append('sexe')\n",
    "    if 'dispo_encoded' in df.columns:\n",
    "        colonnes_a_supprimer.append('dispo')\n",
    "    if 'niveau_experience_encoded' in df.columns:\n",
    "        colonnes_a_supprimer.append('niveau_experience')\n",
    "    if 'categorie_age_encoded' in df.columns:\n",
    "        colonnes_a_supprimer.append('categorie_age')\n",
    "    if 'tranche_salaire_encoded' in df.columns:\n",
    "        colonnes_a_supprimer.append('tranche_salaire')\n",
    "    if 'niveau_note_encoded' in df.columns:\n",
    "        colonnes_a_supprimer.append('niveau_note')\n",
    "    \n",
    "    # Supprimer les colonnes temporelles originales si encod√©es cycliquement\n",
    "    if 'mois_sin' in df.columns and 'mois' in df.columns:\n",
    "        colonnes_a_supprimer.append('mois')\n",
    "    if 'jour_semaine_sin' in df.columns and 'jour_semaine' in df.columns:\n",
    "        colonnes_a_supprimer.append('jour_semaine')\n",
    "    if 'semaine_annee_sin' in df.columns and 'semaine_annee' in df.columns:\n",
    "        colonnes_a_supprimer.append('semaine_annee')\n",
    "    \n",
    "    # Supprimer la colonne date (d√©j√† extraite en variables temporelles)\n",
    "    if 'date' in df.columns:\n",
    "        colonnes_a_supprimer.append('date')\n",
    "    \n",
    "    # Supprimer la colonne index (non pr√©dictive)\n",
    "    if 'index' in df.columns:\n",
    "        colonnes_a_supprimer.append('index')\n",
    "    \n",
    "    # Supprimer diplome_num si diplome_encoded existe (doublon)\n",
    "    if 'diplome_encoded' in df.columns and 'diplome_num' in df.columns:\n",
    "        colonnes_a_supprimer.append('diplome_num')\n",
    "    \n",
    "    # Filtrer les colonnes qui existent r√©ellement\n",
    "    colonnes_a_supprimer = [col for col in colonnes_a_supprimer if col in df.columns]\n",
    "    \n",
    "    if len(colonnes_a_supprimer) > 0:\n",
    "        df = df.drop(columns=colonnes_a_supprimer)\n",
    "        print(f\"   - {len(colonnes_a_supprimer)} colonnes supprim√©es : {colonnes_a_supprimer}\")\n",
    "    else:\n",
    "        print(f\"   - Aucune colonne √† supprimer\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # R√âCAPITULATIF\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"R√âCAPITULATIF - {nom_base}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nNombre de colonnes APR√àS encodage : {len(df.columns)}\")\n",
    "    \n",
    "    # Compter les types de colonnes\n",
    "    colonnes_numeriques = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    colonnes_categoriques = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nTypes de colonnes :\")\n",
    "    print(f\"   - Num√©riques : {len(colonnes_numeriques)}\")\n",
    "    print(f\"   - Cat√©gorielles restantes : {len(colonnes_categoriques)}\")\n",
    "    \n",
    "    if len(colonnes_categoriques) > 0:\n",
    "        print(f\"     ATTENTION : Colonnes cat√©gorielles non encod√©es : {colonnes_categoriques}\")\n",
    "    \n",
    "    # V√©rifier s'il reste des valeurs manquantes\n",
    "    nb_missing = df.isnull().sum().sum()\n",
    "    print(f\"\\nValeurs manquantes : {nb_missing}\")\n",
    "    \n",
    "    if nb_missing > 0:\n",
    "        print(f\"   ATTENTION : Colonnes avec NaN :\")\n",
    "        print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c31aa1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chargement de df_dropna_features.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_dropna_features.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes AVANT encodage : 35\n",
      "\n",
      "[1/4] Label Encoding pour variables ORDINALES...\n",
      "   - diplome : encod√© selon hi√©rarchie (bac=0, licence=1, master=2, doctorat=3)\n",
      "     R√©partition : {'master': 7176, 'licence': 7061, 'doctorat': 2440, 'bac': 2344}\n",
      "   - niveau_experience : encod√© selon hi√©rarchie (Debutant=0, Expert=3)\n",
      "   - categorie_age : encod√© selon hi√©rarchie (Jeune=0, Senior=3)\n",
      "   - tranche_salaire : encod√© selon hi√©rarchie (Bas=0, Tres_eleve=3)\n",
      "   - niveau_note : encod√© selon hi√©rarchie (Faible=0, Excellent=3)\n",
      "\n",
      "[2/4] Label Encoding pour variables BINAIRES...\n",
      "   - sexe : encod√© (F=0, M=1)\n",
      "     R√©partition : {'M': 11355, 'F': 7666}\n",
      "   - dispo : encod√© (non=0, oui=1)\n",
      "     R√©partition : {'non': 11332, 'oui': 7689}\n",
      "\n",
      "[3/4] One-Hot Encoding pour variables NOMINALES...\n",
      "   - cheveux : 4 modalit√©s -> 3 colonnes binaires\n",
      "     Colonnes cr√©√©es : ['cheveux_brun', 'cheveux_chatain', 'cheveux_roux']\n",
      "   - specialite : 4 modalit√©s d√©tect√©es\n",
      "     Colonnes cr√©√©es : 3\n",
      "\n",
      "[4/4] Encodage CYCLIQUE pour variables TEMPORELLES...\n",
      "   - mois : encodage cyclique (mois_sin, mois_cos)\n",
      "     Justification : d√©cembre (12) est proche de janvier (1)\n",
      "   - jour_semaine : encodage cyclique (jour_semaine_sin, jour_semaine_cos)\n",
      "     Justification : dimanche (6) est proche de lundi (0)\n",
      "   - trimestre : conserv√© tel quel (1, 2, 3, 4)\n",
      "     Note : d√©j√† num√©rique et ordinal\n",
      "   - semaine_annee : encodage cyclique (semaine_annee_sin, semaine_annee_cos)\n",
      "\n",
      "[5/4] Nettoyage : suppression des colonnes originales encod√©es...\n",
      "   - 13 colonnes supprim√©es : ['diplome', 'sexe', 'dispo', 'niveau_experience', 'categorie_age', 'tranche_salaire', 'niveau_note', 'mois', 'jour_semaine', 'semaine_annee', 'date', 'index', 'diplome_num']\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_dropna_features.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes APR√àS encodage : 39\n",
      "\n",
      "Types de colonnes :\n",
      "   - Num√©riques : 33\n",
      "   - Cat√©gorielles restantes : 6\n",
      "     ATTENTION : Colonnes cat√©gorielles non encod√©es : ['cheveux_brun', 'cheveux_chatain', 'cheveux_roux', 'spec_detective', 'spec_forage', 'spec_geologie']\n",
      "\n",
      "Valeurs manquantes : 2\n",
      "   ATTENTION : Colonnes avec NaN :\n",
      "categorie_age_encoded    2\n",
      "dtype: int64\n",
      "\n",
      "Base encod√©e sauvegard√©e : ../data\\df_dropna_encoded.csv\n",
      "\n",
      "Chargement de df_median_mode_features.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_median_mode_features.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes AVANT encodage : 35\n",
      "\n",
      "[1/4] Label Encoding pour variables ORDINALES...\n",
      "   - diplome : encod√© selon hi√©rarchie (bac=0, licence=1, master=2, doctorat=3)\n",
      "     R√©partition : {'master': 7623, 'licence': 7377, 'doctorat': 2547, 'bac': 2453}\n",
      "   - niveau_experience : encod√© selon hi√©rarchie (Debutant=0, Expert=3)\n",
      "   - categorie_age : encod√© selon hi√©rarchie (Jeune=0, Senior=3)\n",
      "   - tranche_salaire : encod√© selon hi√©rarchie (Bas=0, Tres_eleve=3)\n",
      "   - niveau_note : encod√© selon hi√©rarchie (Faible=0, Excellent=3)\n",
      "\n",
      "[2/4] Label Encoding pour variables BINAIRES...\n",
      "   - sexe : encod√© (F=0, M=1)\n",
      "     R√©partition : {'M': 11989, 'F': 8011}\n",
      "   - dispo : encod√© (non=0, oui=1)\n",
      "     R√©partition : {'non': 11954, 'oui': 8046}\n",
      "\n",
      "[3/4] One-Hot Encoding pour variables NOMINALES...\n",
      "   - cheveux : 4 modalit√©s -> 3 colonnes binaires\n",
      "     Colonnes cr√©√©es : ['cheveux_brun', 'cheveux_chatain', 'cheveux_roux']\n",
      "   - specialite : 4 modalit√©s d√©tect√©es\n",
      "     Colonnes cr√©√©es : 3\n",
      "\n",
      "[4/4] Encodage CYCLIQUE pour variables TEMPORELLES...\n",
      "   - mois : encodage cyclique (mois_sin, mois_cos)\n",
      "     Justification : d√©cembre (12) est proche de janvier (1)\n",
      "   - jour_semaine : encodage cyclique (jour_semaine_sin, jour_semaine_cos)\n",
      "     Justification : dimanche (6) est proche de lundi (0)\n",
      "   - trimestre : conserv√© tel quel (1, 2, 3, 4)\n",
      "     Note : d√©j√† num√©rique et ordinal\n",
      "   - semaine_annee : encodage cyclique (semaine_annee_sin, semaine_annee_cos)\n",
      "\n",
      "[5/4] Nettoyage : suppression des colonnes originales encod√©es...\n",
      "   - 13 colonnes supprim√©es : ['diplome', 'sexe', 'dispo', 'niveau_experience', 'categorie_age', 'tranche_salaire', 'niveau_note', 'mois', 'jour_semaine', 'semaine_annee', 'date', 'index', 'diplome_num']\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_median_mode_features.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes APR√àS encodage : 39\n",
      "\n",
      "Types de colonnes :\n",
      "   - Num√©riques : 33\n",
      "   - Cat√©gorielles restantes : 6\n",
      "     ATTENTION : Colonnes cat√©gorielles non encod√©es : ['cheveux_brun', 'cheveux_chatain', 'cheveux_roux', 'spec_detective', 'spec_forage', 'spec_geologie']\n",
      "\n",
      "Valeurs manquantes : 2\n",
      "   ATTENTION : Colonnes avec NaN :\n",
      "categorie_age_encoded    2\n",
      "dtype: int64\n",
      "\n",
      "Base encod√©e sauvegard√©e : ../data\\df_median_mode_encoded.csv\n",
      "\n",
      "Chargement de df_knn_features.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_knn_features.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes AVANT encodage : 35\n",
      "\n",
      "[1/4] Label Encoding pour variables ORDINALES...\n",
      "   - diplome : encod√© selon hi√©rarchie (bac=0, licence=1, master=2, doctorat=3)\n",
      "     R√©partition : {'master': 7604, 'licence': 7387, 'doctorat': 2556, 'bac': 2453}\n",
      "   - niveau_experience : encod√© selon hi√©rarchie (Debutant=0, Expert=3)\n",
      "   - categorie_age : encod√© selon hi√©rarchie (Jeune=0, Senior=3)\n",
      "   - tranche_salaire : encod√© selon hi√©rarchie (Bas=0, Tres_eleve=3)\n",
      "   - niveau_note : encod√© selon hi√©rarchie (Faible=0, Excellent=3)\n",
      "\n",
      "[2/4] Label Encoding pour variables BINAIRES...\n",
      "   - sexe : encod√© (F=0, M=1)\n",
      "     R√©partition : {'M': 11960, 'F': 8040}\n",
      "   - dispo : encod√© (non=0, oui=1)\n",
      "     R√©partition : {'non': 11918, 'oui': 8082}\n",
      "\n",
      "[3/4] One-Hot Encoding pour variables NOMINALES...\n",
      "   - cheveux : 4 modalit√©s -> 3 colonnes binaires\n",
      "     Colonnes cr√©√©es : ['cheveux_brun', 'cheveux_chatain', 'cheveux_roux']\n",
      "   - specialite : 4 modalit√©s d√©tect√©es\n",
      "     Colonnes cr√©√©es : 3\n",
      "\n",
      "[4/4] Encodage CYCLIQUE pour variables TEMPORELLES...\n",
      "   - mois : encodage cyclique (mois_sin, mois_cos)\n",
      "     Justification : d√©cembre (12) est proche de janvier (1)\n",
      "   - jour_semaine : encodage cyclique (jour_semaine_sin, jour_semaine_cos)\n",
      "     Justification : dimanche (6) est proche de lundi (0)\n",
      "   - trimestre : conserv√© tel quel (1, 2, 3, 4)\n",
      "     Note : d√©j√† num√©rique et ordinal\n",
      "   - semaine_annee : encodage cyclique (semaine_annee_sin, semaine_annee_cos)\n",
      "\n",
      "[5/4] Nettoyage : suppression des colonnes originales encod√©es...\n",
      "   - 13 colonnes supprim√©es : ['diplome', 'sexe', 'dispo', 'niveau_experience', 'categorie_age', 'tranche_salaire', 'niveau_note', 'mois', 'jour_semaine', 'semaine_annee', 'date', 'index', 'diplome_num']\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_knn_features.csv\n",
      "================================================================================\n",
      "\n",
      "Nombre de colonnes APR√àS encodage : 39\n",
      "\n",
      "Types de colonnes :\n",
      "   - Num√©riques : 33\n",
      "   - Cat√©gorielles restantes : 6\n",
      "     ATTENTION : Colonnes cat√©gorielles non encod√©es : ['cheveux_brun', 'cheveux_chatain', 'cheveux_roux', 'spec_detective', 'spec_forage', 'spec_geologie']\n",
      "\n",
      "Valeurs manquantes : 730\n",
      "   ATTENTION : Colonnes avec NaN :\n",
      "annee                    91\n",
      "trimestre                91\n",
      "categorie_age_encoded     2\n",
      "mois_sin                 91\n",
      "mois_cos                 91\n",
      "jour_semaine_sin         91\n",
      "jour_semaine_cos         91\n",
      "semaine_annee_sin        91\n",
      "semaine_annee_cos        91\n",
      "dtype: int64\n",
      "\n",
      "Base encod√©e sauvegard√©e : ../data\\df_knn_encoded.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# APPLICATION SUR LES 3 BASES DE DONN√âES\n",
    "# =============================================================================\n",
    "\n",
    "# Chemin du dossier data\n",
    "data_path = '../data'\n",
    "\n",
    "bases = [\n",
    "    'df_dropna_features.csv',\n",
    "    'df_median_mode_features.csv', \n",
    "    'df_knn_features.csv'\n",
    "]\n",
    "\n",
    "for nom_base in bases:\n",
    "    try:\n",
    "        # Chemin complet\n",
    "        chemin_fichier = os.path.join(data_path, nom_base)\n",
    "        \n",
    "        # Charger la base\n",
    "        print(f\"\\nChargement de {nom_base}...\")\n",
    "        df = pd.read_csv(chemin_fichier)\n",
    "        \n",
    "        # Appliquer l'encodage\n",
    "        df_encoded = encoder_variables(df, nom_base)\n",
    "        \n",
    "        # Sauvegarder avec un nouveau nom\n",
    "        nom_output = nom_base.replace('_features.csv', '_encoded.csv')\n",
    "        chemin_output = os.path.join(data_path, nom_output)\n",
    "        df_encoded.to_csv(chemin_output, index=False)\n",
    "        \n",
    "        print(f\"\\nBase encod√©e sauvegard√©e : {chemin_output}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nATTENTION : Fichier non trouv√© : {chemin_fichier}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERREUR lors du traitement de {nom_base} : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a3ed9b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "V√âRIFICATION FINALE\n",
      "================================================================================\n",
      "\n",
      "V√©rification sur df_median_mode_encoded.csv :\n",
      "\n",
      "Dimensions : (20000, 39)\n",
      "   - Lignes : 20000\n",
      "   - Colonnes : 39\n",
      "\n",
      "Types de donn√©es :\n",
      "float64    18\n",
      "int64      15\n",
      "bool        6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Premi√®res colonnes (aper√ßu) :\n",
      "['age', 'exp', 'salaire', 'note', 'embauche', 'annee', 'trimestre', 'exp_age_ratio', 'salaire_par_exp', 'ecart_exp_max', 'diplome_x_exp', 'diplome_x_note', 'exp_x_note', 'score_composite', 'exp_elevee_pour_age', 'salaire_eleve', 'note_elevee', 'diplome_eleve', 'est_senior', 'profil_atypique']\n",
      "\n",
      "Exemple de donn√©es encod√©es :\n",
      "    age   exp  salaire   note  diplome_encoded  sexe_encoded  dispo_encoded  \\\n",
      "0  25.0   9.0  26803.0  97.08                1             0              0   \n",
      "1  35.0  13.0  38166.0  63.86                1             1              0   \n",
      "2  29.0  13.0  35207.0  78.50                1             1              0   \n",
      "3  35.0  12.0  32442.0  45.09                1             1              0   \n",
      "4  35.0   6.0  28533.0  81.91                1             0              0   \n",
      "5  37.0   8.0  38558.0  63.46                2             1              0   \n",
      "6  33.0  12.0  39476.0  50.20                2             1              1   \n",
      "7  31.0  10.0  42392.0  62.20                1             1              1   \n",
      "8  43.0  10.0  28625.0  65.17                3             1              0   \n",
      "9  28.0  11.0  32454.0  66.93                2             1              0   \n",
      "\n",
      "   embauche  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "5         1  \n",
      "6         0  \n",
      "7         0  \n",
      "8         1  \n",
      "9         1  \n",
      "\n",
      "SUCCES : Toutes les variables sont num√©riques !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# V√âRIFICATION FINALE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"V√âRIFICATION FINALE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Charger une des bases encod√©es pour v√©rification\n",
    "try:\n",
    "    chemin_verif = os.path.join(data_path, 'df_median_mode_encoded.csv')\n",
    "    df_verif = pd.read_csv(chemin_verif)\n",
    "    \n",
    "    print(f\"\\nV√©rification sur df_median_mode_encoded.csv :\")\n",
    "    print(f\"\\nDimensions : {df_verif.shape}\")\n",
    "    print(f\"   - Lignes : {df_verif.shape[0]}\")\n",
    "    print(f\"   - Colonnes : {df_verif.shape[1]}\")\n",
    "    \n",
    "    print(f\"\\nTypes de donn√©es :\")\n",
    "    print(df_verif.dtypes.value_counts())\n",
    "    \n",
    "    print(f\"\\nPremi√®res colonnes (aper√ßu) :\")\n",
    "    print(df_verif.columns.tolist()[:20])\n",
    "    \n",
    "    print(f\"\\nExemple de donn√©es encod√©es :\")\n",
    "    colonnes_affichage = ['age', 'exp', 'salaire', 'note', 'diplome_encoded', \n",
    "                         'sexe_encoded', 'dispo_encoded', 'embauche']\n",
    "    colonnes_disponibles = [col for col in colonnes_affichage if col in df_verif.columns]\n",
    "    print(df_verif[colonnes_disponibles].head(10))\n",
    "    \n",
    "    # V√©rifier qu'il ne reste aucune colonne cat√©gorielle non encod√©e\n",
    "    colonnes_object = df_verif.select_dtypes(include=['object']).columns.tolist()\n",
    "    if len(colonnes_object) > 0:\n",
    "        print(f\"\\nATTENTION : Colonnes de type 'object' (non num√©riques) restantes :\")\n",
    "        print(colonnes_object)\n",
    "    else:\n",
    "        print(f\"\\nSUCCES : Toutes les variables sont num√©riques !\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nImpossible de charger le fichier de v√©rification : {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dedf24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BILAN FINAL DE L'ENCODAGE\n",
      "================================================================================\n",
      "\n",
      "Encodage termin√© avec succ√®s !\n",
      "\n",
      "Fichiers g√©n√©r√©s :\n",
      "   - ../data/df_dropna_encoded.csv\n",
      "   - ../data/df_median_mode_encoded.csv\n",
      "   - ../data/df_knn_encoded.csv\n",
      "\n",
      "M√©thodes d'encodage appliqu√©es :\n",
      "   [1] Label Encoding (ordinales) :\n",
      "       - diplome (bac=0, licence=1, master=2, doctorat=3)\n",
      "       - niveau_experience, categorie_age, tranche_salaire, niveau_note\n",
      "\n",
      "   [2] Label Encoding (binaires) :\n",
      "       - sexe (F=0, M=1)\n",
      "       - dispo (non=0, oui=1)\n",
      "\n",
      "   [3] One-Hot Encoding (nominales) :\n",
      "       - cheveux (drop_first=True)\n",
      "       - specialite (avec regroupement si > 15 modalit√©s)\n",
      "\n",
      "   [4] Encodage cyclique (temporelles) :\n",
      "       - mois (mois_sin, mois_cos)\n",
      "       - jour_semaine (jour_semaine_sin, jour_semaine_cos)\n",
      "       - semaine_annee (semaine_annee_sin, semaine_annee_cos)\n",
      "\n",
      "Colonnes supprim√©es :\n",
      "   - Versions textuelles des variables encod√©es\n",
      "   - date (remplac√©e par variables temporelles)\n",
      "   - index (non pr√©dictive)\n",
      "\n",
      "================================================================================\n",
      "PHASE 2.3 TERMIN√âE - Pr√™t pour la normalisation/standardisation !\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BILAN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BILAN FINAL DE L'ENCODAGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nEncodage termin√© avec succ√®s !\\n\")\n",
    "\n",
    "print(f\"Fichiers g√©n√©r√©s :\")\n",
    "print(f\"   - ../data/df_dropna_encoded.csv\")\n",
    "print(f\"   - ../data/df_median_mode_encoded.csv\")\n",
    "print(f\"   - ../data/df_knn_encoded.csv\")\n",
    "\n",
    "print(f\"\\nM√©thodes d'encodage appliqu√©es :\")\n",
    "print(f\"   [1] Label Encoding (ordinales) :\")\n",
    "print(f\"       - diplome (bac=0, licence=1, master=2, doctorat=3)\")\n",
    "print(f\"       - niveau_experience, categorie_age, tranche_salaire, niveau_note\")\n",
    "print(f\"\\n   [2] Label Encoding (binaires) :\")\n",
    "print(f\"       - sexe (F=0, M=1)\")\n",
    "print(f\"       - dispo (non=0, oui=1)\")\n",
    "print(f\"\\n   [3] One-Hot Encoding (nominales) :\")\n",
    "print(f\"       - cheveux (drop_first=True)\")\n",
    "print(f\"       - specialite (avec regroupement si > 15 modalit√©s)\")\n",
    "print(f\"\\n   [4] Encodage cyclique (temporelles) :\")\n",
    "print(f\"       - mois (mois_sin, mois_cos)\")\n",
    "print(f\"       - jour_semaine (jour_semaine_sin, jour_semaine_cos)\")\n",
    "print(f\"       - semaine_annee (semaine_annee_sin, semaine_annee_cos)\")\n",
    "\n",
    "print(f\"\\nColonnes supprim√©es :\")\n",
    "print(f\"   - Versions textuelles des variables encod√©es\")\n",
    "print(f\"   - date (remplac√©e par variables temporelles)\")\n",
    "print(f\"   - index (non pr√©dictive)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2.3 TERMIN√âE - Pr√™t pour la normalisation/standardisation !\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e43e03",
   "metadata": {},
   "source": [
    "#### SECTION 5 : NORMALISATION / STANDARDISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e07c5",
   "metadata": {},
   "source": [
    "Objectif : Standardiser les variables num√©riques pour les algorithmes sensibles\n",
    "M√©thode : StandardScaler (moyenne=0, √©cart-type=1)\n",
    "\n",
    "Note : Cette normalisation sera appliqu√©e GLOBALEMENT sur les 3 bases.\n",
    "       Lors de la mod√©lisation, il faudra refaire un fit() sur train uniquement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "155b2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FONCTION DE NORMALISATION\n",
    "# =============================================================================\n",
    "\n",
    "def normaliser_variables(df_input, nom_base):\n",
    "    \"\"\"\n",
    "    Standardise les variables num√©riques (moyenne=0, √©cart-type=1)\n",
    "    \n",
    "    Param√®tres :\n",
    "    - df_input : DataFrame √† normaliser\n",
    "    - nom_base : nom du fichier pour l'affichage\n",
    "    \n",
    "    Retourne :\n",
    "    - DataFrame normalis√©\n",
    "    - Scaler ajust√© (pour r√©utilisation)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Traitement de : {nom_base}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. IDENTIFICATION DES VARIABLES √Ä NORMALISER\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n[1/3] Identification des variables √† normaliser...\")\n",
    "    \n",
    "    # UNIQUEMENT LE SALAIRE sera normalis√©\n",
    "    variables_a_normaliser = ['salaire']\n",
    "    \n",
    "    # V√©rifier que la variable existe\n",
    "    if 'salaire' not in df.columns:\n",
    "        print(\"   ERREUR : La variable 'salaire' n'existe pas dans le dataset\")\n",
    "        return df, None\n",
    "    \n",
    "    print(f\"\\n   Variable √† normaliser : salaire\")\n",
    "    print(f\"   Justification : Seule variable avec une √©chelle tr√®s diff√©rente des autres\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. APPLICATION DU STANDARDSCALER\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(f\"\\n[2/3] Application de StandardScaler...\")\n",
    "    \n",
    "    if len(variables_a_normaliser) == 0:\n",
    "        print(\"   ATTENTION : Aucune variable √† normaliser d√©tect√©e\")\n",
    "        return df, None\n",
    "    \n",
    "    # Cr√©er le scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Afficher les statistiques AVANT normalisation\n",
    "    print(f\"\\n   Statistiques du SALAIRE AVANT normalisation :\")\n",
    "    stats_avant = df[['salaire']].describe().loc[['mean', 'std', 'min', 'max']]\n",
    "    print(stats_avant.round(2))\n",
    "    \n",
    "    # Appliquer la normalisation\n",
    "    df[variables_a_normaliser] = scaler.fit_transform(df[variables_a_normaliser])\n",
    "    \n",
    "    # Afficher les statistiques APR√àS normalisation\n",
    "    print(f\"\\n   Statistiques du SALAIRE APR√àS normalisation :\")\n",
    "    stats_apres = df[['salaire']].describe().loc[['mean', 'std', 'min', 'max']]\n",
    "    print(stats_apres.round(2))\n",
    "    \n",
    "    print(f\"\\n   StandardScaler appliqu√© avec succ√®s\")\n",
    "    print(f\"   - Moyenne des variables normalis√©es : ~0\")\n",
    "    print(f\"   - √âcart-type des variables normalis√©es : ~1\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. V√âRIFICATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(f\"\\n[3/3] V√©rification post-normalisation...\")\n",
    "    \n",
    "    # V√©rifier qu'il n'y a pas de valeurs infinies ou NaN cr√©√©es\n",
    "    nb_inf = np.isinf(df['salaire']).sum()\n",
    "    nb_nan = df['salaire'].isnull().sum()\n",
    "    \n",
    "    print(f\"\\n   Valeurs infinies cr√©√©es : {nb_inf}\")\n",
    "    print(f\"   Valeurs NaN cr√©√©es : {nb_nan}\")\n",
    "    \n",
    "    if nb_inf > 0 or nb_nan > 0:\n",
    "        print(f\"   ATTENTION : Probl√®me d√©tect√© lors de la normalisation\")\n",
    "    else:\n",
    "        print(f\"   SUCCES : Aucune valeur aberrante cr√©√©e\")\n",
    "    \n",
    "    # V√©rifier que les autres variables n'ont pas √©t√© modifi√©es\n",
    "    print(f\"\\n   Variables NON normalis√©es (conserv√©es telles quelles) :\")\n",
    "    variables_non_modifiees = ['age', 'exp', 'note']\n",
    "    variables_disponibles = [col for col in variables_non_modifiees if col in df.columns]\n",
    "    if len(variables_disponibles) > 0:\n",
    "        print(df[variables_disponibles].describe().loc[['mean', 'min', 'max']].round(2))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # R√âCAPITULATIF\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"R√âCAPITULATIF - {nom_base}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\n   Variable normalis√©e : salaire\")\n",
    "    print(f\"   Toutes les autres variables : conserv√©es telles quelles\")\n",
    "    print(f\"   M√©thode : StandardScaler (moyenne=0, √©cart-type=1)\")\n",
    "    \n",
    "    return df, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "86ff47d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLICATION SUR LES 3 BASES DE DONN√âES\n",
      "================================================================================\n",
      "\n",
      "Chargement de df_dropna_encoded.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_dropna_encoded.csv\n",
      "================================================================================\n",
      "\n",
      "[1/3] Identification des variables √† normaliser...\n",
      "\n",
      "   Variable √† normaliser : salaire\n",
      "   Justification : Seule variable avec une √©chelle tr√®s diff√©rente des autres\n",
      "\n",
      "[2/3] Application de StandardScaler...\n",
      "\n",
      "   Statistiques du SALAIRE AVANT normalisation :\n",
      "       salaire\n",
      "mean  34966.88\n",
      "std    5005.21\n",
      "min   14128.00\n",
      "max   53977.00\n",
      "\n",
      "   Statistiques du SALAIRE APR√àS normalisation :\n",
      "      salaire\n",
      "mean     0.00\n",
      "std      1.00\n",
      "min     -4.16\n",
      "max      3.80\n",
      "\n",
      "   StandardScaler appliqu√© avec succ√®s\n",
      "   - Moyenne des variables normalis√©es : ~0\n",
      "   - √âcart-type des variables normalis√©es : ~1\n",
      "\n",
      "[3/3] V√©rification post-normalisation...\n",
      "\n",
      "   Valeurs infinies cr√©√©es : 0\n",
      "   Valeurs NaN cr√©√©es : 0\n",
      "   SUCCES : Aucune valeur aberrante cr√©√©e\n",
      "\n",
      "   Variables NON normalis√©es (conserv√©es telles quelles) :\n",
      "       age   exp    note\n",
      "mean  35.0   9.5   72.97\n",
      "min    0.0   0.0    8.68\n",
      "max   72.0  23.0  100.00\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_dropna_encoded.csv\n",
      "================================================================================\n",
      "\n",
      "   Variable normalis√©e : salaire\n",
      "   Toutes les autres variables : conserv√©es telles quelles\n",
      "   M√©thode : StandardScaler (moyenne=0, √©cart-type=1)\n",
      "\n",
      "Base normalis√©e sauvegard√©e : ../data\\df_dropna_normalized.csv\n",
      "Scaler sauvegard√© : ../data\\df_dropna_scaler.pkl\n",
      "\n",
      "Chargement de df_median_mode_encoded.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_median_mode_encoded.csv\n",
      "================================================================================\n",
      "\n",
      "[1/3] Identification des variables √† normaliser...\n",
      "\n",
      "   Variable √† normaliser : salaire\n",
      "   Justification : Seule variable avec une √©chelle tr√®s diff√©rente des autres\n",
      "\n",
      "[2/3] Application de StandardScaler...\n",
      "\n",
      "   Statistiques du SALAIRE AVANT normalisation :\n",
      "       salaire\n",
      "mean  34974.37\n",
      "std    4991.20\n",
      "min   14128.00\n",
      "max   53977.00\n",
      "\n",
      "   Statistiques du SALAIRE APR√àS normalisation :\n",
      "      salaire\n",
      "mean     0.00\n",
      "std      1.00\n",
      "min     -4.18\n",
      "max      3.81\n",
      "\n",
      "   StandardScaler appliqu√© avec succ√®s\n",
      "   - Moyenne des variables normalis√©es : ~0\n",
      "   - √âcart-type des variables normalis√©es : ~1\n",
      "\n",
      "[3/3] V√©rification post-normalisation...\n",
      "\n",
      "   Valeurs infinies cr√©√©es : 0\n",
      "   Valeurs NaN cr√©√©es : 0\n",
      "   SUCCES : Aucune valeur aberrante cr√©√©e\n",
      "\n",
      "   Variables NON normalis√©es (conserv√©es telles quelles) :\n",
      "        age   exp    note\n",
      "mean  35.01   9.5   72.97\n",
      "min    0.00   0.0    8.68\n",
      "max   74.00  23.0  100.00\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_median_mode_encoded.csv\n",
      "================================================================================\n",
      "\n",
      "   Variable normalis√©e : salaire\n",
      "   Toutes les autres variables : conserv√©es telles quelles\n",
      "   M√©thode : StandardScaler (moyenne=0, √©cart-type=1)\n",
      "\n",
      "Base normalis√©e sauvegard√©e : ../data\\df_median_mode_normalized.csv\n",
      "Scaler sauvegard√© : ../data\\df_median_mode_scaler.pkl\n",
      "\n",
      "Chargement de df_knn_encoded.csv...\n",
      "\n",
      "================================================================================\n",
      "Traitement de : df_knn_encoded.csv\n",
      "================================================================================\n",
      "\n",
      "[1/3] Identification des variables √† normaliser...\n",
      "\n",
      "   Variable √† normaliser : salaire\n",
      "   Justification : Seule variable avec une √©chelle tr√®s diff√©rente des autres\n",
      "\n",
      "[2/3] Application de StandardScaler...\n",
      "\n",
      "   Statistiques du SALAIRE AVANT normalisation :\n",
      "       salaire\n",
      "mean  34974.91\n",
      "std    4993.39\n",
      "min   14128.00\n",
      "max   53977.00\n",
      "\n",
      "   Statistiques du SALAIRE APR√àS normalisation :\n",
      "      salaire\n",
      "mean    -0.00\n",
      "std      1.00\n",
      "min     -4.18\n",
      "max      3.81\n",
      "\n",
      "   StandardScaler appliqu√© avec succ√®s\n",
      "   - Moyenne des variables normalis√©es : ~0\n",
      "   - √âcart-type des variables normalis√©es : ~1\n",
      "\n",
      "[3/3] V√©rification post-normalisation...\n",
      "\n",
      "   Valeurs infinies cr√©√©es : 0\n",
      "   Valeurs NaN cr√©√©es : 0\n",
      "   SUCCES : Aucune valeur aberrante cr√©√©e\n",
      "\n",
      "   Variables NON normalis√©es (conserv√©es telles quelles) :\n",
      "        age   exp    note\n",
      "mean  35.01   9.5   72.96\n",
      "min    0.00   0.0    8.68\n",
      "max   74.00  23.0  100.00\n",
      "\n",
      "================================================================================\n",
      "R√âCAPITULATIF - df_knn_encoded.csv\n",
      "================================================================================\n",
      "\n",
      "   Variable normalis√©e : salaire\n",
      "   Toutes les autres variables : conserv√©es telles quelles\n",
      "   M√©thode : StandardScaler (moyenne=0, √©cart-type=1)\n",
      "\n",
      "Base normalis√©e sauvegard√©e : ../data\\df_knn_normalized.csv\n",
      "Scaler sauvegard√© : ../data\\df_knn_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLICATION SUR LES 3 BASES DE DONN√âES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLICATION SUR LES 3 BASES DE DONN√âES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Chemin du dossier data\n",
    "data_path = '../data'\n",
    "\n",
    "bases = [\n",
    "    'df_dropna_encoded.csv',\n",
    "    'df_median_mode_encoded.csv', \n",
    "    'df_knn_encoded.csv'\n",
    "]\n",
    "\n",
    "scalers = {}  # Pour sauvegarder les scalers\n",
    "\n",
    "for nom_base in bases:\n",
    "    try:\n",
    "        # Chemin complet\n",
    "        chemin_fichier = os.path.join(data_path, nom_base)\n",
    "        \n",
    "        # Charger la base\n",
    "        print(f\"\\nChargement de {nom_base}...\")\n",
    "        df = pd.read_csv(chemin_fichier)\n",
    "        \n",
    "        # Appliquer la normalisation\n",
    "        df_normalized, scaler = normaliser_variables(df, nom_base)\n",
    "        \n",
    "        # Sauvegarder le dataset normalis√©\n",
    "        nom_output = nom_base.replace('_encoded.csv', '_normalized.csv')\n",
    "        chemin_output = os.path.join(data_path, nom_output)\n",
    "        df_normalized.to_csv(chemin_output, index=False)\n",
    "        \n",
    "        print(f\"\\nBase normalis√©e sauvegard√©e : {chemin_output}\")\n",
    "        \n",
    "        # Sauvegarder le scaler\n",
    "        if scaler is not None:\n",
    "            nom_scaler = nom_base.replace('_encoded.csv', '_scaler.pkl')\n",
    "            chemin_scaler = os.path.join(data_path, nom_scaler)\n",
    "            joblib.dump(scaler, chemin_scaler)\n",
    "            scalers[nom_base] = scaler\n",
    "            print(f\"Scaler sauvegard√© : {chemin_scaler}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nATTENTION : Fichier non trouv√© : {chemin_fichier}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERREUR lors du traitement de {nom_base} : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
